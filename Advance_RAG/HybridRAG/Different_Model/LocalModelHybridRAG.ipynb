{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b0a3a40-d436-4c6d-a6ff-614612ce6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  pypdf langchain langchain-community langchain-core langchain-experimental langchain-text-splitters langchain-ollama pinecone-client pinecone-text pinecone-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6975cdc-7b7b-4ba3-86ca-4bab03138dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo:latest\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d3c79-b4c9-4a8f-b0d1-526c3fe5c398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f872608-0cc7-4026-8c30-04fce90d7338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shubhamrathod/PycharmProjects/RAG_Pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1611867-c054-4e93-bb53-8e24ddb075a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fdc849b-8343-431c-b3d0-8a482ad41c41",
   "metadata": {},
   "source": [
    "# HYBRID RAG With Local Model and PineCone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ad5e6-5161-433e-b5ec-e11805e42b56",
   "metadata": {},
   "source": [
    "## Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dbc6713-4ec5-4b1d-9ac9-cd1b9875dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = f'{HOME}/yolov10.pdf'\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c929143-c9d5-47b8-a589-fde65aa03526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d3370ba-5bdc-4476-8fb1-7a484bf5d652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/shubhamrathod/PycharmProjects/RAG_Pipeline/yolov10.pdf', 'page': 0}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37dc67-fc2d-489f-8af8-d95c79681bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19db6ab4-6171-48f6-a973-ac87fbe0420e",
   "metadata": {},
   "source": [
    "# Split the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a99e733-7fe3-4d22-b6e3-eb1283114179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=3000,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6aed96f-b640-4c49-ac47-731f4c3993a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6428ff72-4f6c-4e19-be59-ba554aef77c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34e34337-e67a-46b3-8ec2-f4c52f3d94ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/shubhamrathod/PycharmProjects/RAG_Pipeline/yolov10.pdf', 'page': 0}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0afbab-ddc5-43c9-bcca-31036f915034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21e53d-8163-4ab3-a9e3-d0ffc1a1033a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64c546f6-1948-46c7-9e26-59567415142f",
   "metadata": {},
   "source": [
    "# Setting Up Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fee44b55-b0cd-47cc-b292-22199b351f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dea1009-3137-4d4a-b2e8-73623ad134b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "index_name = \"langchain-pinecone-hybrid-search\"\n",
    "\n",
    "# initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# create the index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,  # dimensionality of dense model\n",
    "        metric=\"dotproduct\",  # sparse values supported only for dotproduct\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b40986d6-1433-424f-872f-52c00bfe8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5a20d63-fd3f-4ff7-8be8-173d8b062596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x13088aec0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82da4f1-f230-4086-a22c-4bea7936aa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483270f-3b02-4c37-9818-2b17906976b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "465fa947-ec53-46d3-a224-948fcfee7934",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b4504-bb14-445b-ac38-4ed36b015216",
   "metadata": {},
   "source": [
    "### Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b22ee2dd-82ed-4fae-ba2f-b845d916ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbba76a-643f-4367-a722-64e787870b7f",
   "metadata": {},
   "source": [
    "### Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aea4f069-53e2-4450-865b-3b4b72242d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To encode the text to sparse values you can either choose SPLADE or BM25\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE\n",
    "\n",
    "# use default tf-idf values\n",
    "bm25_encoder = BM25Encoder().default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb881ca8-b6da-479e-93a1-9a7ea8772467",
   "metadata": {},
   "source": [
    "### The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9870abb6-99df-4df9-8bf0-07faf8355c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for doc in docs:\n",
    "    content = doc.page_content\n",
    "    corpus.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8aba9cd-a57e-4030-9e6e-85f82f82b069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6986df78-368f-4e4c-8482-535c53a78ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91b9282f-bd33-48f7-9066-92da0f640691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shubhamrathod/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/shubhamrathod/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this only if error pop up while running below code\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bddb9ce-882e-4ca5-b969-3e53d06e5346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 72.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit tf-idf values on your corpus\n",
    "bm25_encoder.fit(corpus)\n",
    "\n",
    "# store the values to a json file\n",
    "bm25_encoder.dump(\"bm25_values.json\")\n",
    "\n",
    "# load to your BM25Encoder object\n",
    "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc58176-572a-4d3f-945f-26740c52a0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2ccd29-25e0-4c59-9a90-9ca7043c944e",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec323e63-4824-4178-b747-48485a43a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "\n",
    "retriever = PineconeHybridSearchRetriever(\n",
    "    embeddings=embeddings,\n",
    "    sparse_encoder=bm25_encoder,\n",
    "    index=index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bcac4e6-5529-4af9-83a6-e2fae0a1a91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PineconeHybridSearchRetriever(embeddings=OllamaEmbeddings(model='mxbai-embed-large:latest'), sparse_encoder=<pinecone_text.sparse.bm25_encoder.BM25Encoder object at 0x1315abf70>, index=<pinecone.data.index.Index object at 0x13088aec0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e13c0-017e-454b-a686-86567d8b0373",
   "metadata": {},
   "source": [
    "## Use Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59c8aac8-c6c1-4e3c-b8d9-89310dc1c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever.add_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17ebf6d4-1107-4857-9bc4-c83dcf3fb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.invoke(\"Implementation Details of Yolov10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98379084-c5e0-47e2-a69b-1948cc7ba876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='mixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, e.g., embedded in the\\nELAN structure [ 58,20] (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to\\nachieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we\\nsort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance\\nvariation of replacing the basic block in the leading stage with CIB. If there is no performance\\ndegradation compared with the given model, we proceed with the replacement of the next stage and\\nhalt the process otherwise. Consequently, we can implement adaptive compact block designs across\\nstages and model scales, achieving higher efficiency without compromising performance. Due to the\\npage limit, we provide the details of the algorithm in the appendix.\\nAccuracy driven model design. We further explore the large-kernel convolution and self-attention\\nfor accuracy driven design, aiming to boost the performance under minimal cost.\\n(1) Large-kernel convolution. Employing large-kernel depthwise convolution is an effective way to\\nenlarge the receptive field and enhance the model’s capability [ 9,38,37]. However, simply leveraging\\nthem in all stages may introduce contamination in shallow features used for detecting small objects,\\nwhile also introducing significant I/Ooverhead and latency in high-resolution stages [ 7]. Therefore,\\nwe propose to leverage the large-kernel depthwise convolutions in CIB within the deep stages.\\nSpecifically, we increase the kernel size of the second 3 ×3 depthwise convolution in the CIB to 7 ×7,\\nfollowing [ 37]. Additionally, we employ the structural reparameterization technique [ 10,9,53] to\\nbring another 3 ×3 depthwise convolution branch to alleviate the optimization issue without inference\\noverhead. Furthermore, as the model size increases, its receptive field naturally expands, with\\nthe benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.\\n(2) Partial self-attention (PSA). Self-attention [ 52] is widely employed in various visual tasks due\\nto its remarkable global modeling capability [ 36,13,70]. However, it exhibits high computational\\ncomplexity and memory footprint. To address this, in light of the prevalent attention head redun-\\ndancy [ 63], we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c).\\nSpecifically, we evenly partition the features across channels into two parts after the 1 ×1 convolution.\\nWe only feed one part into the NPSAblocks comprised of multi-head self-attention module (MHSA)\\nand feed-forward network (FFN). Two parts are then concatenated and fused by a 1 ×1 convolution.\\nBesides, we follow [ 21] to assign the dimensions of the query and key to half of that of the value in\\nMHSA and replace the LayerNorm [1] with BatchNorm [26] for fast inference. Furthermore, PSA is\\nonly placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the\\nquadratic computational complexity of self-attention. In this way, the global representation learning\\nability can be incorporated into YOLOs with low computational costs, which well enhances the\\nmodel’s capability and leads to improved performance.\\n4 Experiments\\n4.1 Implementation Details\\nWe select YOLOv8 [ 20] as our baseline model, due to its commendable latency-accuracy balance\\nand its availability in various model sizes. We employ the consistent dual assignments for NMS-free\\ntraining and perform holistic efficiency-accuracy driven model design based on it, which brings our\\nYOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\\nverify the proposed detector on COCO [ 33] under the same training-from-scratch setting [ 20,59,56].\\nMoreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [71].\\n4.2 Comparison with state-of-the-arts\\nAs shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency\\nacross various model scales. We first compare YOLOv10 with our baseline models, i.e., YOLOv8.\\nOn N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP\\nimprovements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38%\\nless calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,\\nfor lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0\\n6'),\n",
       " Document(page_content='A Appendix\\nA.1 Implementation Details\\nFollowing [ 20,56,59], all YOLOv10 models are trained from scratch using the SGD optimizer for\\n500 epochs. The SGD momentum and weight decay are set to 0.937 and 5 ×10−4, respectively. The\\ninitial learning rate is 1 ×10−2and it decays linearly to 1 ×10−4. For data augmentation, we adopt the\\nMosaic [ 2,19], Mixup [ 68] and copy-paste augmentation [ 17],etc., like [ 20,59]. Tab. 14 presents the\\ndetailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase\\nthe width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the\\nSPPF module [ 20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion\\nratio of 2 for the inverted bottleneck block structure. Following [ 59,56], we report the standard mean\\naverage precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\\nMoreover, we follow [ 71] to establish the end-to-end speed benchmark. Since the execution time\\nof NMS is affected by the input, we thus measure the latency on the COCO valset, like [ 71]. We\\nadopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT\\nefficientNMSPlugin is appended for post-processing and the I/Ooverhead is omitted. We report\\nthe average latency across all images.\\nTable 14: Hyper-parameters of YOLOv10.\\nhyper-parameter YOLOv10-N/S/M/B/L/X\\nepochs 500\\noptimizer SGD\\nmomentum 0.937\\nweight decay 5 ×10−4\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\ninitial learning rate 10−2\\nfinal learning rate 10−4\\nlearning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015\\ntranslation augmentation 0.1\\nscale augmentation 0.5/0.5/0.9/0.9/0.9/0.9\\nmosaic augmentation 1.0\\nMixup augmentation 0.0/0.0/0.1/0.1/0.15/0.15\\ncopy-paste augmentation 0.0/0.0/0.1/0.1/0.3/0.3\\nclose mosaic epochs 10\\nA.2 Details of Consistent Matching Metric\\nWe provide the detailed derivation of consistent matching metric here.\\nAs mentioned in the paper, we suppose that the one-to-many positive samples is Ωand the one-to-\\none branch selects i-th prediction. We can then leverage the normalized metric [ 14] to obtain the\\nclassification target for task alignment learning [20, 14, 59, 27, 64], i.e.,to2m,j=u∗·mo2m,j\\nm∗\\no2m≤u∗\\nforj∈Ωandto2o,i=u∗·mo2o,i\\nm∗\\no2o=u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\\nA=|(1−to2o,i)−(1−I(i∈Ω)to2m,i)|+X\\nk∈Ω\\\\{i}|1−(1−to2m,k)|\\n=|to2o,i−I(i∈Ω)to2m,i|+X\\nk∈Ω\\\\{i}to2m,k\\n=to2o,i−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k,(3)\\n15'),\n",
       " Document(page_content='Algorithm 1: Rank-guided block design\\nInput: Intrinsic ranks Rfor all stages S; Original Network Θ; CIB θcib;\\nOutput: New network Θ∗with CIB for certain stages.\\n1t←0;\\n2Θ0←Θ;Θ∗←Θ0;\\n3ap0←AP(T(Θ0)); // T:training the network; AP:evaluating the AP performance.\\n4while S̸=∅do\\n5 st←argmins∈SR;\\n6 Θt+1←Replace (Θt, θcib,st);// Replace the block in Stage stofΘtwith CIB θcib.\\n7 apt+1←AP(T(Θt+1));\\n8 ifapt+1≥ap0then\\n9 Θ∗←Θt+1;S←S\\\\ {st};\\n10 else\\n11 return Θ∗;\\n12 end\\n13end\\n14return Θ∗;\\nTable 15: Detailed performance of YOLOv10 on COCO.\\nModel APval(%) APval\\n50(%) APval\\n75(%) APval\\nsmall (%) APval\\nmedium (%) APval\\nlarge (%)\\nYOLOv10-N 38.5 53.8 41.7 18.9 42.4 54.6\\nYOLOv10-S 46.3 63.0 50.4 26.8 51.0 63.8\\nYOLOv10-M 51.1 68.1 55.8 33.8 56.5 67.0\\nYOLOv10-B 52.5 69.6 57.2 35.1 57.8 68.5\\nYOLOv10-L 53.2 70.1 58.1 35.8 58.5 69.4\\nYOLOv10-X 54.4 71.3 59.3 37.0 59.8 70.9\\n0.05ms latency overhead. Besides, for YOLOv10-M (#6 in Tab. 2), which has a larger model scale\\nand more redundancy, our efficiency driven model design results in a considerable 12.5% latency\\nreduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a\\nnotable 0.8% AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48ms.\\nThese results well demonstrate the effectiveness of our design strategy across different model scales.\\nA.6 Visualization Results\\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It\\ncan be observed that YOLOv10 can achieve precise detection under various difficult conditions, such\\nas low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely\\npacked objects, such as bottle, cup, and person. These results indicate its superior performance.\\nA.7 Contribution, Limitation, and Broader Impact\\nContribution. In summary, our contributions are three folds as follows:\\n1.We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label\\nassignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious\\nsupervision between two branches, we innovatively propose the consistent matching metric, which\\ncan well reduce the theoretical supervision gap and lead to improved performance.\\n2.We propose a holistic efficiency-accuracy driven model design strategy for the model architecture\\nof YOLOs. We present novel lightweight classification head, spatial-channel decoupled down-\\nsampling, and rank-guided block design, which greatly reduce the computational redundancy and\\nachieve high efficiency. We further introduce the large-kernel convolution and innovative partial\\nself-attention module, which effectively enhance the performance under low cost.\\n3.Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object\\ndetector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art\\nperformance and efficiency trade-offs compared with other advanced detectors.\\n17'),\n",
       " Document(page_content='Figure 4: Visualization results under complex and challenging scenarios.\\nLimitation. Due to the limited computational resources, we do not investigate the pretraining\\nof YOLOv10 on large-scale datasets, e.g., Objects365 [ 47]. Besides, although we can achieve\\ncompetitive end-to-end performance using the one-to-one head under NMS-free training, there still\\nexists a performance gap compared with the original one-to-many training using NMS, especially\\nnoticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of\\none-to-many training with NMS outperforms that of NMS-free training by 1.0% AP and 0.5% AP,\\nrespectively. We will explore ways to further reduce the gap and achieve higher performance for\\nYOLOv10 in the future work.\\nBroader impact. The YOLOs can be widely applied in various real-world applications, including\\nmedical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\\nfields and improve the efficiency. However, we acknowledge the potential for malicious use of our\\nmodels. We will make every effort to prevent this.\\n18')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88470cd-5d83-4683-85ef-0ad70fcca9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ace170e-aeff-46e1-af59-6fd0429e0899",
   "metadata": {},
   "source": [
    "# Chain\n",
    "Memory - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec4fd5b2-7e4e-4ad8-a150-abe30a841b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e7f4657-c8be-4b28-abc0-2d11dc765c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo:latest\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe13962-a31e-410f-bee0-eeb6412bf629",
   "metadata": {},
   "source": [
    "## Defining System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7627332a-87c8-4eb0-8e9b-6bf32bd837fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"\n",
    "      You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer\n",
    "      the question. Base your respose only based on this context, If you don't know the answer, say that you don't know. Use three sentences maximum and keep the\n",
    "      answer concise.\n",
    "      \\n\\n\n",
    "      {context}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8813a7b-4bc1-4166-b78d-ea12582c9772",
   "metadata": {},
   "source": [
    "## Contextualizing the Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aac44923-d8c6-47d9-922a-aedf3512219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"\"\"\n",
    "    Given a chat history and the latest user question which might reference context in the chat history,\n",
    "    formulate a standalone question which can be understood without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "606386e8-2c51-4f65-9a7c-fba0d9426c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm,\n",
    "    retriever,\n",
    "    contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb598d-015b-4f65-bb8b-32b7baaa3ff9",
   "metadata": {},
   "source": [
    "## Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "619f07ca-ace0-470e-98d6-4048f8a2a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709b606-6b52-46a7-80d4-85cb533de708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0148be21-19f7-4553-a10b-7b1ab597a985",
   "metadata": {},
   "source": [
    "# Run Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c53047b6-1340-4160-aec5-9201a99f1e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"YOLO\" can stand for several things, depending on the context. (1) In popular culture, it's often used as an acronym for \"You Only Live Once,\" which encourages living life to its fullest without regrets. Here are a few other meanings:\n",
      "\n",
      "- **In computing and artificial intelligence:**\n",
      "  - **YOLO** is also the name of a real-time object detection system introduced in 2016 by Joseph Redmon et al. It's known for its speed and accuracy, making it popular in applications like self-driving cars and security cameras.\n",
      "  - **YOLOv3**, **YOLOv4**, **YOLOv5**, etc., are successive versions of this system.\n",
      "\n",
      "- **In social media:**\n",
      "  - #Yolo is sometimes used as a hashtag to accompany posts about living life adventurously or making spontaneous decisions.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Yolo?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76219795-72ba-4fd9-9b6e-879d28508c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yolov10 is a real-time object detector introduced in the paper \"YOLOv10: A New Real-Time End-to-End Object Detector\" by Li et al. Here are some key implementation details of Yolov10:\n",
      "\n",
      "1. **Consistent Dual Assignments Strategy**:\n",
      "   - **One-to-Many Branch (Training)**: During training, each ground truth object is assigned to multiple anchors with the highest IoU overlap. This provides rich supervision.\n",
      "   - **One-to-One Branch (Inference)**: During inference, each predicted box is assigned to a single ground truth object based on the maximum IoU overlap. This ensures high efficiency.\n",
      "   - **Consistent Matching Metric**: To reduce the supervision gap between training and inference, Yolov10 introduces a consistent matching metric that considers both branches during training.\n",
      "\n",
      "2. **Efficiency-Accuracy Driven Model Design**:\n",
      "   - **Lightweight Classification Head**: Yolov10 uses a lightweight classification head with fewer parameters to reduce computational redundancy.\n",
      "   - **Spatial-Channel Decoupled Down-Sampling**: Instead of using pooling layers, Yolov10 employs spatial and channel decoupled down-sampling to preserve more information.\n",
      "   - **Rank-Guided Block Design**: Yolov10 introduces rank-guided blocks that dynamically adjust the number of channels based on their importance, further reducing redundancy.\n",
      "\n",
      "3. **Large-Kernel Convolution**: Yolov10 uses large-kernel convolutions (7x7 and 9x9) to capture more contextual information with fewer parameters compared to smaller kernels.\n",
      "\n",
      "4. **Partial Self-Attention Module**: To enhance performance under low computational cost, Yolov10 introduces a partial self-attention module that selectively attends to relevant features.\n",
      "\n",
      "5. **Model Scales**: Yolov10 comes in different scales (N, S, M, B, L, X) with varying numbers of layers and parameters, offering trade-offs between accuracy and speed:\n",
      "   - N: 24 layers, 3.7M parameters\n",
      "   - S: 36 layers, 9.5M parameters\n",
      "   - M: 48 layers, 15.9M parameters\n",
      "   - B: 60 layers, 23.6M parameters\n",
      "   - L: 72 layers, 33.4M parameters\n",
      "   - X: 84 layers, 46.2M parameters\n",
      "\n",
      "6. **Training**:\n",
      "   - Yolov10 is trained using the standard YOLO training procedure with some modifications to accommodate the consistent dual assignments strategy.\n",
      "   - The authors use data augmentation techniques such as random horizontal and vertical flipping, random translation, scaling, rotation, and color jittering.\n",
      "\n",
      "7. **Inference**:\n",
      "   - During inference, Yolov10 first generates candidate boxes using the one-to-one branch. Then, it refines these boxes using the one-to-many branch to improve accuracy.\n",
      "   - The final detections are obtained by applying a non-maximum suppression (NMS) algorithm with an IoU threshold of 0.45.\n",
      "\n",
      "These implementation details contribute to Yolov10's state-of-the-art performance and efficiency trade-offs compared to other advanced detectors.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the implemenattion detail of Yolov10?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e701d94a-8a23-45fa-a916-0e84501e6932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text from the YOLOv10 paper, here are the key conclusions:\n",
      "\n",
      "1. **Superior Performance and Efficiency**: YOLOv10 outperforms other state-of-the-art object detectors like YOLOv8, RT-DETR, Gold-YOLO, and YOLOv9 in terms of Average Precision (AP) while having fewer parameters and lower computational costs.\n",
      "\n",
      "2. **Real-Time Detection**: YOLOv10 demonstrates significant improvements in inference speed compared to other methods. For instance, YOLOv10-S/X achieves 1.8x/1.3x faster inference speed than RT-DETR-R18/R101 under similar performance.\n",
      "\n",
      "3. **Effectiveness of Architectural Designs**: The paper shows that the architectural designs used in YOLOv10, such as NMS-free training with consistent dual assignments and efficiency-driven model design, lead to substantial reductions in latency and improvements in AP.\n",
      "\n",
      "4. **State-of-the-Art Performance Across Different Model Scales**: When compared using the original one-to-many training approach, YOLOv10 exhibits state-of-the-art performance and efficiency across different model scales.\n",
      "\n",
      "5. **Significant Improvements with Minimal Overhead**: The accuracy-driven model design in YOLOv10 achieves notable improvements in AP with only a small latency overhead (0.18ms for YOLOv10-S and 0.17ms for YOLOv10-M).\n",
      "\n",
      "In conclusion, the YOLOv10 paper presents a new version of the YOLO family of object detectors that offers superior performance and efficiency, making it well-suited for real-time end-to-end detection tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the conclusion we obtain from  Yolov10 paper?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f0239270-65b9-447c-81d8-d55313e6068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided text, YOLOv10 outperforms YOLOv6-3.0-N/S by 1.5 AP and 2.0 AP at small/medium object sizes respectively. Here's a summary of the performance comparison:\n",
      "\n",
      "| Model | AP (small) | AP (medium) |\n",
      "|---|---|---|\n",
      "| YOLOv6-3.0-N/S | ? | ? |\n",
      "| **YOLOv10** | **+1.5** | **+2.0** |\n",
      "\n",
      "However, without the specific AP values for YOLOv6-3.0-N/S, we cannot determine by how much YOLOv10 outperforms them numerically. The text only mentions the improvement in terms of Average Precision (AP) points.\n",
      "\n",
      "Additionally, the paper mentions that YOLOv10 achieves state-of-the-art performance and efficiency trade-offs compared with other advanced detectors, but it doesn't explicitly mention which specific models it outperforms besides YOLOv6-3.0-N/S. To get a complete list of outperformed models, you would need to refer to the full paper or the detailed performance tables provided in the text (Table 15).\n"
     ]
    }
   ],
   "source": [
    "question = \"Based on the paper, Yolov10 outperforms how many other model and which are those?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3360e-503d-4e88-a3a1-605615c85d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrag",
   "language": "python",
   "name": "myrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
