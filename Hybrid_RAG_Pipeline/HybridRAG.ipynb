{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-EHCjVGPnET"
   },
   "source": [
    "# Hybrid Search:\n",
    "\n",
    "**Combination of**\n",
    "\n",
    "\n",
    "1.   Semantic Search -> Dense Vector Search\n",
    "2.   Syntatic Search -> Sparse Vector Search -> Exact Search -> Keyword Search.\n",
    "\n",
    "We use Reciprocal Rank Fusion In Hybrid Search\n",
    "\n",
    "## Reciprocal Rank Fusion (RRF):\n",
    "The RRF score is calculated by taking the sum of the reciprocal rankings that is given from each list. By putting the rank of the document in the denominator, it penalizes the documents that are ranked lower in the list.\n",
    "\n",
    "[HYBRID SEARCH](https://www.youtube.com/watch?v=CK0ExcCWDP4&t=2230s)\n",
    "\n",
    "[RERANK](https://www.youtube.com/watch?v=Uh9bYiVrW_s&list=PLuqoMuyL16cXOMakHBMk_GMRob85yNOV5&index=119)\n",
    "\n",
    "[HYBRID+RERANK](https://medium.com/@nadikapoudel16/advanced-rag-implementation-using-hybrid-search-reranking-with-zephyr-alpha-llm-4340b55fef22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rU5PZ2xOPIBN"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  pypdf langchain langchain-community langchain-core langchain-experimental langchain-text-splitters langchain-openai pinecone-client pinecone-text pinecone-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LpW8OqlfO-us"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ['OPENAI_API_KEY']= userdata.get('OPENAI_API_KEY')\n",
    "pinecone = userdata.get('pinecone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVzQq6eCgZQV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3cSROSfkhsq"
   },
   "source": [
    "# Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "60I-bh-vcXhf"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = '/content/yolov10.pdf'\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6-zk8hNczHB",
    "outputId": "0abbbb10-488f-4446-dec0-515a13c5183e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/content/yolov10.pdf', 'page': 0}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxEgwoLWdfOk",
    "outputId": "aefdf811-8210-4950-81f7-fa32f4130d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxpFV7kyd2GK",
    "outputId": "a205bc70-de9d-44b3-e9d3-bc96b0c76780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPfoB000gadb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSbAHcNtkkcC"
   },
   "source": [
    "# Split the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "_aVdizGddLcE"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=3000,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "s0ZtQ9nAdlcP"
   },
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lED1Ky4d3xG",
    "outputId": "602df16e-77e3-4bad-d708-df78d30cdf5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/content/yolov10.pdf', 'page': 0}, page_content='YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KJ91h6yd4hY",
    "outputId": "2bd32fbf-1b18-4df4-d8b1-4b6cbcb58a5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vWw9HwfgVFs",
    "outputId": "a60fa646-9263-48fc-dbde-3f74fa936d19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "oGnTDD_MgbuN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "oqmSCwlygboy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qUp_KZzDgbl5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bepLFZ3NeSCr"
   },
   "source": [
    "# Setting Up Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "GH0KHntpeOTI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "index_name = \"langchain-pinecone-hybrid-search\"\n",
    "\n",
    "# initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone)\n",
    "\n",
    "# create the index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # dimensionality of dense model\n",
    "        metric=\"dotproduct\",  # sparse values supported only for dotproduct\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Lp5KStmzeRCc"
   },
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oE2De7LfgZb",
    "outputId": "4d4ac199-bb52-497f-8424-85137dd88d3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x7ce410204d90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZjFAUXYgcl2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY-CKc9DgcjP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AavgMF2Bfp2J"
   },
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiGLFkgyim-T"
   },
   "source": [
    "### Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0pWOSeOSfl0A"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kojlfIylip4W"
   },
   "source": [
    "### Sparese Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Jwz8bwBlfurS"
   },
   "outputs": [],
   "source": [
    "# To encode the text to sparse values you can either choose SPLADE or BM25\n",
    "\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE\n",
    "\n",
    "# use default tf-idf values\n",
    "bm25_encoder = BM25Encoder().default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kadx8Y_JgRMi"
   },
   "source": [
    "## The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZIsIiDAh6D4",
    "outputId": "cea83cd1-9c55-4500-c8b3-da6ab53d8a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024', '1 Introduction\\nReal-time object detection has always been a focal point of research in the area of computer vision,\\nwhich aims to accurately predict the categories and positions of objects in an image under low\\nlatency. It is widely adopted in various practical applications, including autonomous driving [ 3],\\nrobot navigation [ 11], and object tracking [ 66],etc. In recent years, researchers have concentrated\\non devising CNN-based object detectors to achieve real-time detection [ 18,22,43,44,45,51,\\n12]. Among them, YOLOs have gained increasing popularity due to their adept balance between\\nperformance and efficiency [ 2,19,27,19,20,59,54,64,7,65,16,27]. The detection pipeline of\\nYOLOs consists of two parts: the model forward process and the NMS post-processing. However,\\nboth of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby\\none ground-truth object corresponds to multiple positive samples. Despite yielding superior perfor-\\nmance, this approach necessitates NMS to select the best positive prediction during inference. This\\nslows down the inference speed and renders the performance sensitive to the hyperparameters of NMS,\\nthereby preventing YOLOs from achieving optimal end-to-end deployment [ 71]. One line to tackle\\nthis issue is to adopt the recently introduced end-to-end DETR architectures [ 4,74,67,28,34,40,61].\\nFor example, RT-DETR [ 71] presents an efficient hybrid encoder and uncertainty-minimal query\\nselection, propelling DETRs into the realm of real-time applications. Nevertheless, the inherent\\ncomplexity in deploying DETRs impedes its ability to attain the optimal balance between accuracy\\nand speed. Another line is to explore end-to-end detection for CNN-based detectors, which typically\\nleverages one-to-one assignment strategies to suppress the redundant predictions [ 5,49,60,73,16].\\nHowever, they usually introduce additional inference overhead or achieve suboptimal performance.\\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which\\nexhibits an important impact on the accuracy and speed [ 45,16,65,7]. To achieve more efficient\\nand effective model architectures, researchers have explored different design strategies. Various\\nprimary computational units are presented for the backbone to enhance the feature extraction ability,\\nincluding DarkNet [ 43,44,45], CSPNet [ 2], EfficientRep [ 27] and ELAN [ 56,58],etc. For the neck,\\nPAN [ 35], BiC [ 27], GD [ 54] and RepGFPN [ 65],etc., are explored to enhance the multi-scale feature\\nfusion. Besides, model scaling strategies [ 56,55] and re-parameterization [ 10,27] techniques are also\\ninvestigated. While these efforts have achieved notable advancements, a comprehensive inspection for\\nvarious components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As\\na result, there still exists considerable computational redundancy within YOLOs, leading to inefficient\\nparameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability\\nalso leads to inferior performance, leaving ample room for accuracy improvements.\\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of\\nYOLOs. We target both the post-processing and the model architecture throughout the detection\\npipeline. To this end, we first tackle the problem of redundant predictions in the post-processing\\nby presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label\\nassignments and consistent matching metric. It allows the model to enjoy rich and harmonious\\nsupervision during training while eliminating the need for NMS during inference, leading to com-\\npetitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy\\ndriven model design strategy for the model architecture by performing the comprehensive inspection\\nfor various components in YOLOs. For efficiency, we propose the lightweight classification head,\\nspatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested\\ncomputational redundancy and achieve more efficient architecture. For accuracy, we explore the\\nlarge-kernel convolution and present the effective partial self-attention module to enhance the model\\ncapability, harnessing the potential for performance improvements under low cost.\\nBased on these approaches, we succeed in achieving a new family of real-time end-to-end detectors\\nwith different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard\\nbenchmarks for object detection, i.e., COCO [ 33], demonstrate that our YOLOv10 can significantly\\noutperform previous state-of-the-art models in terms of computation-accuracy trade-offs across\\nvarious model scales. As shown in Fig. 1, our YOLOv10-S / X are 1.8 ×/ 1.3×faster than RT-DETR-\\nR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B\\nachieves a 46% reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly\\nefficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and\\n0.5 AP, with 1.8 ×and 2.3 ×smaller number of parameters, respectively. YOLOv10-M achieves the\\n2', 'similar AP compared with YOLOv9-M / YOLO-MS, with 23% / 31% fewer parameters, respectively.\\nWe hope that our work can inspire further studies and advancements in the field.\\n2 Related Work\\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low\\nlatency, which is crucial for real-world applications. Over the past years, substantial efforts have been\\ndirected towards developing efficient detectors [ 18,51,43,32,72,69,30,29,39]. Particularly, the\\nYOLO series [ 43,44,45,2,19,27,56,20,59] stand out as the mainstream ones. YOLOv1, YOLOv2,\\nand YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck,\\nand head [43, 44, 45]. YOLOv4 [2] and YOLOv5 [19] introduce the CSPNet [57] design to replace\\nDarkNet [ 42], coupled with data augmentation strategies, enhanced PAN, and a greater variety of\\nmodel scales, etc. YOLOv6 [ 27] presents BiC and SimCSPSPPF for neck and backbone, respectively,\\nwith anchor-aided training and self-distillation strategy. YOLOv7 [ 56] introduces E-ELAN for rich\\ngradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [ 20] presents C2f\\nbuilding block for effective feature extraction and fusion. Gold-YOLO [ 54] provides the advanced\\nGD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [ 59] proposes GELAN to\\nimprove the architecture and PGI to augment the training process.\\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from\\ntraditional pipelines, offering streamlined architectures [ 48]. DETR [ 4] introduces the transformer\\narchitecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminat-\\ning hand-crafted components and post-processing. Since then, various DETR variants have been\\nproposed to enhance its performance and efficiency [ 40,61,50,28,34]. Deformable-DETR [ 74]\\nleverages multi-scale deformable attention module to accelerate the convergence speed. DINO [ 67]\\nintegrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs.\\nRT-DETR [ 71] further designs the efficient hybrid encoder and proposes the uncertainty-minimal\\nquery selection to improve both the accuracy and latency. Another line to achieve end-to-end object\\ndetection is based CNN detectors. Learnable NMS [ 23] and relation networks [ 25] present another\\nnetwork to remove duplicated predictions for detectors. OneNet [49] and DeFCN [60] propose one-\\nto-one matching strategies to enable end-to-end object detection with fully convolutional networks.\\nFCOS pss[73] introduces a positive sample selector to choose the optimal sample for prediction.\\n3 Methodology\\n3.1 Consistent Dual Assignments for NMS-free Training\\nDuring training, YOLOs [ 20,59,27,64] usually leverage TAL [ 14] to allocate multiple positive sam-\\nples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals,\\nfacilitating the optimization and achieving superior performance. However, it necessitates YOLOs\\nto rely on the NMS post-processing, which causes the suboptimal inference efficiency for deploy-\\nment. While previous works [ 49,60,73,5] explore one-to-one matching to suppress the redundant\\npredictions, they usually introduce additional inference overhead or yield suboptimal performance.\\nIn this work, we present a NMS-free training strategy for YOLOs with dual label assignments and\\nconsistent matching metric, achieving both high efficiency and competitive performance.\\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one\\nprediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak\\nsupervision, which causes suboptimal accuracy and convergence speed [ 75]. Fortunately, this\\ndeficiency can be compensated for by the one-to-many assignment [ 5]. To achieve this, we introduce\\ndual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown\\nin Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure\\nand adopts the same optimization objectives as the original one-to-many branch but leverages the\\none-to-one matching to obtain label assignments. During training, two heads are jointly optimized\\nwith the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-\\nto-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one\\nhead to make predictions. This enables YOLOs for the end-to-end deployment without incurring any\\nadditional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which\\nachieves the same performance as Hungarian matching [4] with less extra training time.\\n3', 'Backbone PANRegression\\nClassificationOne-to-many HeadDual Label Assignments Consistent Match. Metric\\n𝑚=𝑠⋅𝑝𝛼⋅IoU \\u0de0𝑏,𝑏𝛽 \\nRegression\\nClassificationOne-to-one Head\\nInput\\n(a) (b)\\n1\\n2 34\\n6\\n5Figure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one\\nassignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs αo2m=0.5 and\\nβo2m=6 by default [20]. For consistency, αo2o=0.5; βo2o=6. For inconsistency, αo2o=0.5; βo2o=2.\\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches\\nleverage a metric to quantitatively assess the level of concordance between predictions and instances.\\nTo achieve prediction aware matching for both branches, we employ a uniform matching metric, i.e.,\\nm(α, β) =s·pα·IoU(ˆb, b)β, (1)\\nwhere pis the classification score, ˆbandbdenote the bounding box of prediction and instance,\\nrespectively. srepresents the spatial prior indicating whether the anchor point of prediction is within\\nthe instance [ 20,59,27,64].αandβare two important hyperparameters that balance the impact\\nof the semantic prediction task and the location regression task. We denote the one-to-many and\\none-to-one metrics as mo2m=m(αo2m, βo2m)andmo2o=m(αo2o, βo2o), respectively. These metrics\\ninfluence the label assignments and supervision information for the two heads.\\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than\\none-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that\\nof one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many\\nhead’s optimization. As a result, the one-to-one head can provide improved quality of samples during\\ninference, leading to better performance. To this end, we first analyze the supervision gap between the\\ntwo heads. Due to the randomness during training, we initiate our examination in the beginning with\\ntwo heads initialized with the same values and producing the same predictions, i.e., one-to-one head\\nand one-to-many head generate the same pand IoU for each prediction-instance pair. We note that the\\nregression targets of two branches do not conflict, as matched predictions share the same targets and\\nunmatched predictions are ignored. The supervision gap thus lies in the different classification targets.\\nGiven an instance, we denote its largest IoU with predictions as u∗, and the largest one-to-many and\\none-to-one matching scores as m∗\\no2mandm∗\\no2o, respectively. Suppose that one-to-many branch yields\\nthe positive samples Ωand one-to-one branch selects i-th prediction with the metric mo2o,i=m∗\\no2o, we\\ncan then derive the classification target to2m,j=u∗·mo2m,j\\nm∗\\no2m≤u∗forj∈Ωandto2o,i=u∗·mo2o,i\\nm∗\\no2o=u∗\\nfor task aligned loss as in [ 20,59,27,64,14]. The supervision gap between two branches can thus\\nbe derived by the 1-Wasserstein distance [41] of different classification objectives, i.e.,\\nA=to2o,i−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k, (2)\\nWe can observe that the gap decreases as to2m,iincreases, i.e.,iranks higher within Ω. It reaches the\\nminimum when to2m,i=u∗,i.e.,iis the best positive sample in Ω, as shown in Fig. 2.(a). To achieve\\nthis, we present the consistent matching metric, i.e.,αo2o=r·αo2mandβo2o=r·βo2m, which implies\\nmo2o=mr\\no2m. Therefore, the best positive sample for one-to-many head is also the best for one-to-one\\nhead. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we\\ntaker=1, by default, i.e.,αo2o=αo2mandβo2o=βo2m. To verify the improved supervision alignment,\\nwe count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results\\nafter training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric.\\nFor a more comprehensive understanding of the mathematical proof, please refer to the appendix.\\n3.2 Holistic Efficiency-Accuracy Driven Model Design\\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the\\nefficiency-accuracy trade-offs [ 45,7,27]. Although previous works explore various design strategies,\\n4', '1×1\\nC×𝑁+3×3 DW\\n1×1\\n3×3 DW\\n1×1\\n3×3 DWSplit\\n1×1CIBCIB1×1\\nSplit\\nCMHSA\\n+FFN+\\n1×1×𝑁𝑝𝑠𝑎\\n(a) (b) (c)Figure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone\\nand neck is numbered in the order of model forward process. The numerical rank ris normalized\\ntor/Cofor y-axis and its threshold is set to λmax/2, by default, where Codenotes the number of\\noutput channels and λmax is the largest singular value. It can be observed that deep stages and large\\nmodels exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial\\nself-attention module (PSA).\\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the\\nmodel architecture exhibits non-negligible computational redundancy and constrained capability,\\nwhich impedes its potential for achieving high efficiency and performance. Here, we aim to holistically\\nperform model designs for YOLOs from both efficiency and accuracy perspectives.\\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling\\nlayers, stages with basic building blocks, and the head. The stem incurs few computational cost and\\nwe thus perform efficiency driven model design for other three parts.\\n(1) Lightweight classification head. The classification and regression heads usually share the same\\narchitecture in YOLOs. However, they exhibit notable disparities in computational overhead. For\\nexample, the FLOPs and parameter count of the classification head (5.95G/1.51M) are 2.5 ×and 2.4 ×\\nthose of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing\\nthe impact of classification error and the regression error (seeing Tab. 6), we find that the regression\\nhead undertakes more significance for the performance of YOLOs. Consequently, we can reduce the\\noverhead of classification head without worrying about hurting the performance greatly. Therefore,\\nwe simply adopt a lightweight architecture for the classification head, which consists of two depthwise\\nseparable convolutions [24, 8] with the kernel size of 3 ×3 followed by a 1 ×1 convolution.\\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular 3 ×3 standard\\nconvolutions with stride of 2, achieving spatial downsampling (from H×WtoH\\n2×W\\n2) and channel\\ntransformation (from Cto2C) simultaneously. This introduces non-negligible computational cost of\\nO(9\\n2HWC2)and parameter count of O(18C2). Instead, we propose to decouple the spatial reduction\\nand channel increase operations, enabling more efficient downsampling. Specifically, we firstly\\nleverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise\\nconvolution to perform spatial downsampling. This reduces the computational cost to O(2HWC2+\\n9\\n2HWC )and the parameter count to O(2C2+18C). Meanwhile, it maximizes information retention\\nduring downsampling, leading to competitive performance with latency reduction.\\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [ 27,\\n59],e.g., the bottleneck block in YOLOv8 [ 20]. To thoroughly examine such homogeneous design for\\nYOLOs, we utilize the intrinsic rank [ 31,15] to analyze the redundancy2of each stage. Specifically,\\nwe calculate the numerical rank of the last convolution in the last basic block in each stage, which\\ncounts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of\\nYOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This\\nobservation suggests that simply applying the same block design for all stages is suboptimal for the\\nbest capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme\\nwhich aims to decrease the complexity of stages that are shown to be redundant using compact\\narchitecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap\\ndepthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\\n2A lower rank implies greater redundancy, while a higher rank signifies more condensed information.\\n5', 'mixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, e.g., embedded in the\\nELAN structure [ 58,20] (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to\\nachieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we\\nsort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance\\nvariation of replacing the basic block in the leading stage with CIB. If there is no performance\\ndegradation compared with the given model, we proceed with the replacement of the next stage and\\nhalt the process otherwise. Consequently, we can implement adaptive compact block designs across\\nstages and model scales, achieving higher efficiency without compromising performance. Due to the\\npage limit, we provide the details of the algorithm in the appendix.\\nAccuracy driven model design. We further explore the large-kernel convolution and self-attention\\nfor accuracy driven design, aiming to boost the performance under minimal cost.\\n(1) Large-kernel convolution. Employing large-kernel depthwise convolution is an effective way to\\nenlarge the receptive field and enhance the model’s capability [ 9,38,37]. However, simply leveraging\\nthem in all stages may introduce contamination in shallow features used for detecting small objects,\\nwhile also introducing significant I/Ooverhead and latency in high-resolution stages [ 7]. Therefore,\\nwe propose to leverage the large-kernel depthwise convolutions in CIB within the deep stages.\\nSpecifically, we increase the kernel size of the second 3 ×3 depthwise convolution in the CIB to 7 ×7,\\nfollowing [ 37]. Additionally, we employ the structural reparameterization technique [ 10,9,53] to\\nbring another 3 ×3 depthwise convolution branch to alleviate the optimization issue without inference\\noverhead. Furthermore, as the model size increases, its receptive field naturally expands, with\\nthe benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.\\n(2) Partial self-attention (PSA). Self-attention [ 52] is widely employed in various visual tasks due\\nto its remarkable global modeling capability [ 36,13,70]. However, it exhibits high computational\\ncomplexity and memory footprint. To address this, in light of the prevalent attention head redun-\\ndancy [ 63], we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c).\\nSpecifically, we evenly partition the features across channels into two parts after the 1 ×1 convolution.\\nWe only feed one part into the NPSAblocks comprised of multi-head self-attention module (MHSA)\\nand feed-forward network (FFN). Two parts are then concatenated and fused by a 1 ×1 convolution.\\nBesides, we follow [ 21] to assign the dimensions of the query and key to half of that of the value in\\nMHSA and replace the LayerNorm [1] with BatchNorm [26] for fast inference. Furthermore, PSA is\\nonly placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the\\nquadratic computational complexity of self-attention. In this way, the global representation learning\\nability can be incorporated into YOLOs with low computational costs, which well enhances the\\nmodel’s capability and leads to improved performance.\\n4 Experiments\\n4.1 Implementation Details\\nWe select YOLOv8 [ 20] as our baseline model, due to its commendable latency-accuracy balance\\nand its availability in various model sizes. We employ the consistent dual assignments for NMS-free\\ntraining and perform holistic efficiency-accuracy driven model design based on it, which brings our\\nYOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\\nverify the proposed detector on COCO [ 33] under the same training-from-scratch setting [ 20,59,56].\\nMoreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [71].\\n4.2 Comparison with state-of-the-arts\\nAs shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency\\nacross various model scales. We first compare YOLOv10 with our baseline models, i.e., YOLOv8.\\nOn N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP\\nimprovements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38%\\nless calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,\\nfor lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0\\n6', 'Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models.\\nLatencyfdenotes the latency in the forward process of model without post-processing. †means the\\nresults of YOLOv10 with the original one-to-many training using NMS. All results below are without\\nthe additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\\nModel #Param.(M) FLOPs(G) APval(%) Latency(ms) Latencyf(ms)\\nYOLOv6-3.0-N [27] 4.7 11.4 37.0 2.69 1.76\\nGold-YOLO-N [54] 5.6 12.1 39.6 2.92 1.82\\nYOLOv8-N [20] 3.2 8.7 37.3 6.16 1.77\\nYOLOv10-N (Ours) 2.3 6.7 38.5 /39.5†1.84 1.79\\nYOLOv6-3.0-S [27] 18.5 45.3 44.3 3.42 2.35\\nGold-YOLO-S [54] 21.5 46.0 45.4 3.82 2.73\\nYOLO-MS-XS [7] 4.5 17.4 43.4 8.23 2.80\\nYOLO-MS-S [7] 8.1 31.2 46.2 10.12 4.83\\nYOLOv8-S [20] 11.2 28.6 44.9 7.07 2.33\\nYOLOv9-S [59] 7.1 26.4 46.7 - -\\nRT-DETR-R18 [71] 20.0 60.0 46.5 4.58 4.49\\nYOLOv10-S (Ours) 7.2 21.6 46.3 /46.8†2.49 2.39\\nYOLOv6-3.0-M [27] 34.9 85.8 49.1 5.63 4.56\\nGold-YOLO-M [54] 41.3 87.5 49.8 6.38 5.45\\nYOLO-MS [7] 22.2 80.2 51.0 12.41 7.30\\nYOLOv8-M [20] 25.9 78.9 50.6 9.50 5.09\\nYOLOv9-M [59] 20.0 76.3 51.1 - -\\nRT-DETR-R34 [71] 31.0 92.0 48.9 6.32 6.21\\nRT-DETR-R50m [71] 36.0 100.0 51.3 6.90 6.84\\nYOLOv10-M (Ours) 15.4 59.1 51.1 /51.3†4.74 4.63\\nYOLOv6-3.0-L [27] 59.6 150.7 51.8 9.02 7.90\\nGold-YOLO-L [54] 75.1 151.7 51.8 10.65 9.78\\nYOLOv9-C [59] 25.3 102.1 52.5 10.57 6.13\\nYOLOv10-B (Ours) 19.1 92.0 52.5 /52.7†5.74 5.67\\nYOLOv8-L [20] 43.7 165.2 52.9 12.39 8.06\\nRT-DETR-R50 [71] 42.0 136.0 53.1 9.20 9.07\\nYOLOv10-L (Ours) 24.4 120.3 53.2 /53.4†7.28 7.21\\nYOLOv8-X [20] 68.2 257.8 53.9 16.86 12.83\\nRT-DETR-R101 [71] 76.0 259.0 54.3 13.71 13.58\\nYOLOv10-X (Ours) 29.5 160.4 54.4 /54.4†10.70 10.60\\nAP, with 51% / 61% fewer parameters and 41% / 52% less computations, respectively. For medium\\nmodels, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46% / 62% latency\\nreduction under the same or better performance, respectively. For large models, compared with\\nGold-YOLO-L, our YOLOv10-L shows 68% fewer parameters and 32% lower latency, along with\\na significant improvement of 1.4% AP. Furthermore, compared with RT-DETR, YOLOv10 obtains\\nsignificant performance and latency improvements. Notably, YOLOv10-S / X achieves 1.8 ×and\\n1.3×faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance.\\nThese results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach.\\nWe consider the performance and the latency of model forward process (Latencyf) in this situation,\\nfollowing [ 56,20,54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance\\nand efficiency across different model scales, indicating the effectiveness of our architectural designs.\\n4.3 Model Analyses\\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It\\ncan be observed that our NMS-free training with consistent dual assignments significantly reduces\\nthe end-to-end latency of YOLOv10-S by 4.63ms, while maintaining competitive performance of\\n44.3% AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters\\nand 20.8 GFlOPs, with a considerable latency reduction of 0.65ms for YOLOv10-M, well showing\\nits effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements\\nof 1.8 AP and 0.7 AP for YOLOv10-S and YOLOv10-M, alone with only 0.18ms and 0.17ms latency\\noverhead, respectively, which well demonstrates its superiority.\\n7', 'Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\\n# Model NMS-free. Efficiency. Accuracy. #Param.(M) FLOPs(G) APval(%) Latency(ms)\\n1\\nYOLOv10-S11.2 28.6 44.9 7.07\\n2 ✓ 11.2 28.6 44.3 2.44\\n3 ✓ ✓ 6.2 20.8 44.5 2.31\\n4 ✓ ✓ ✓ 7.2 21.6 46.3 2.49\\n5\\nYOLOv10-M25.9 78.9 50.6 9.50\\n6 ✓ 25.9 78.9 50.3 5.22\\n7 ✓ ✓ 14.1 58.1 50.4 4.57\\n8 ✓ ✓ ✓ 15.4 59.1 51.1 4.74\\nTable 3: Dual assign.\\no2m o2o AP Latency\\n✓ 44.9 7.07\\n✓43.4 2.44\\n✓✓44.3 2.44Table 4: Matching metric.\\nαo2oβo2oAPvalαo2oβo2oAPval\\n0.5 2.0 42.7 0.25 3.0 44.3\\n0.5 4.0 44.2 0.25 6.0 43.5\\n0.5 6.0 44.3 1.0 6.0 43.9\\n0.5 8.0 44.0 1.012.0 44.3Table 5: Efficiency. for YOLOv10-S/M.\\n# Model #Param FLOPs APvalLatency\\n1 base. 11.2/25.9 28.6/78.9 44.3/50.3 2.44/5.22\\n2 +cls. 9.9/23.2 23.5/67.7 44.2/50.2 2.39/5.07\\n3 +downs. 8.0/19.7 22.2/65.0 44.4/50.4 2.36/4.97\\n4 +block. 6.2/14.1 20.8/58.1 44.5/50.4 2.31/4.57\\nAnalyses for NMS-free training.\\n•Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can\\nbring both rich supervision of one-to-many (o2m) branch during training and high efficiency of\\none-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2. Specifically, we introduce baselines for training with only o2m branch and only o2o branch,\\nrespectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\\n•Consistent matching metric. We introduce consistent matching metric to make the one-to-one head\\nmore harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., #1 in\\nTab. 2, under different αo2oandβo2o. As shown in Tab. 4, the proposed consistent matching metric,\\ni.e.,αo2o=r·αo2mandβo2o=r·βo2m, can achieve the optimal performance, where αo2m=0.5and\\nβo2m=6.0in the one-to-many head [ 20]. Such an improvement can be attributed to the reduction\\nof the supervision gap (Eq. (2)), which provides improved supervision alignment between two\\nbranches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive\\nhyper-parameter tuning, which is appealing in practical scenarios.\\nAnalyses for efficiency driven model design . We conduct experiments to gradually incorporate the\\nefficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nwithout efficiency-accuracy driven model design, i.e., #2/#6 in Tab. 2. As shown in Tab. 5, each design\\ncomponent, including lightweight classification head, spatial-channel decoupled downsampling, and\\nrank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency.\\nImportantly, these improvements are achieved while maintaining competitive performance.\\n•Lightweight classification head. We analyze the impact of category and localization errors of\\npredictions on the performance, based on the YOLOv10-S of #1 and #2 in Tab. 5, like [ 6].\\nSpecifically, we match the predictions to the instances by the one-to-one assignment. Then,\\nwe substitute the predicted category score with instance labels, resulting in APval\\nw/o cwith no\\nclassification errors. Similarly, we replace the predicted locations with those of instances, yielding\\nAPval\\nw/o rwith no regression errors. As shown in Tab. 6, APval\\nw/o ris much higher than APval\\nw/o c,\\nrevealing that eliminating the regression errors achieves greater improvement. The performance\\nbottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification\\nhead can allow higher efficiency without compromising the performance.\\n•Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency,\\nwhere the channel dimensions are first increased by pointwise convolution (PW) and the resolution\\nis then reduced by depthwise convolution (DW) for maximal information retention. We compare it\\nwith the baseline way of spatial reduction by DW followed by channel modulation by PW, based\\non the YOLOv10-S of #3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the\\n0.7% AP improvement by enjoying less information loss during downsampling.\\n•Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its\\neffectiveness based on the YOLOv10-S of #4 in the Tab. 5. Specifically, we introduce the inverted\\nresidual block [ 46] (IRB) as the baseline, which achieves the suboptimal 43.7% AP, as shown in\\nTab. 8. We then append a 3 ×3 depthwise convolution (DW) after it, denoted as “IRB-DW”, which\\n8', 'Table 6: cls. results.\\nbase. +cls.\\nAPval44.3 44.2\\nAPval\\nw/o c 59.9 59.9\\nAPval\\nw/o r 64.5 64.2Table 7: Results of d.s.\\nModel APvalLatency\\nbase. 43.7 2.33\\nours 44.4 2.36Table 8: Results of CIB.\\nModel APvalLatency\\nIRB 43.7 2.30\\nIRB-DW 44.2 2.30\\nours 44.5 2.31Table 9: Rank-guided.\\nStages with CIB APval\\nempty 44.4\\n8 44.5\\n8,4, 44.5\\n8,4,7 44.3\\nTable 10: Accuracy. for S/M.\\n# Model APvalLatency\\n1 base. 44.5/50.4 2.31/4.57\\n2 +L.k. 44.9/- 2.34/-\\n3 +PSA 46.3/51.1 2.49/4.74Table 11: L.k. results.\\nModel APvalLatency\\nk.s.=5 44.7 2.32\\nk.s.=7 44.9 2.34\\nk.s.=9 44.9 2.37\\nw/o rep. 44.8 2.34Table 12: L.k. usage.\\nw/o L.k. w/ L.k.\\nN 36.3 36.6\\nS 44.5 44.9\\nM 50.4 50.4Table 13: PSA results.\\nModel APvalLatency\\nPSA 46.3 2.49\\nTrans. 46.0 2.54\\nNPSA= 1 46.3 2.49\\nNPSA= 2 46.5 2.59\\nbrings 0.5% AP improvement. Compared with “IRB-DW”, our CIB further achieves 0.3% AP\\nimprovement by prepending another DW with minimal overhead, indicating its superiority.\\n•Rank-guided block design. We introduce the rank-guided block design to adaptively integrate\\ncompact block design for improving the model efficiency. We verify its benefit based on the\\nYOLOv10-S of #3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks\\nare Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the\\nbottleneck block in each stage with the efficient CIB, we observe the performance degradation\\nstarting from Stage 7. In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can\\nthus adopt the efficient block design without compromising the performance. These results indicate\\nthat rank-guided block design can serve as an effective strategy for higher model efficiency.\\nAnalyses for accuracy driven model design. We present the results of gradually integrating the\\naccuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model\\nafter incorporating efficiency driven design, i.e., #3/#7 in Tab. 2. As shown in Tab. 10, the adoption\\nof large-kernel convolution and PSA module leads to the considerable performance improvements\\nof 0.4% AP and 1.4% AP for YOLOv10-S under minimal latency increase of 0.03ms and 0.15ms,\\nrespectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\\n•Large-kernel convolution. We first investigate the effect of different kernel sizes based on the\\nYOLOv10-S of #2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size\\nincreases and stagnates around the kernel size of 7 ×7, indicating the benefit of large perception field.\\nBesides, removing the reparameterization branch during training achieves 0.1% AP degradation,\\nshowing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel\\nconvolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no\\nimprovements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We\\nthus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\\n•Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the\\nglobal modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10-\\nS of #3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN,\\nas the baseline, denoted as “Trans.”. As shown in Tab. 13, compared with it, PSA brings 0.3% AP\\nimprovement with 0.05ms latency reduction. The performance enhancement may be attributed\\nto the alleviation of optimization problem [ 62,9] in self-attention, by mitigating the redundancy\\nin attention heads. Moreover, we investigate the impact of different NPSA. As shown in Tab. 13,\\nincreasing NPSAto 2 obtains 0.2% AP improvement but with 0.1ms latency overhead. Therefore,\\nwe set NPSAto 1, by default, to enhance the model capability while maintaining high efficiency.\\n5 Conclusion\\nIn this paper, we target both the post-processing and model architecture throughout the detection\\npipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMS-\\nfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the\\nholistic efficiency-accuracy driven model design strategy, improving the performance-efficiency trade-\\noffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments\\nshow that YOLOv10 achieves the state-of-the-art performance and latency compared with other\\nadvanced detectors, well demonstrating its superiority.\\n9', 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed\\nand accuracy of object detection, 2020.\\n[3]Daniel Bogdoll, Maximilian Nitsche, and J Marius Zöllner. Anomaly detection in autonomous\\ndriving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 4488–4499, 2022.\\n[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In European conference on\\ncomputer vision , pages 213–229. Springer, 2020.\\n[5]Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end\\nfully convolutional object detection. arXiv preprint arXiv:2211.13859 , 2022.\\n[6]Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing\\nyour trained detrs with box refinement. arXiv preprint arXiv:2307.11828 , 2023.\\n[7]Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng.\\nYolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv\\npreprint arXiv:2308.05480 , 2023.\\n[8]François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed-\\nings of the IEEE conference on computer vision and pattern recognition , pages 1251–1258,\\n2017.\\n[9]Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to\\n31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 11963–11975, 2022.\\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.\\nRepvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 13733–13742, 2021.\\n[11] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel\\nFernando Tello Gamarra. Mobile robot navigation using an object recognition software with\\nrgbd images and the yolo algorithm. Applied Artificial Intelligence , 33(14):1290–1305, 2019.\\n[12] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet:\\nKeypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference\\non computer vision , pages 6569–6578, 2019.\\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 12873–12883, 2021.\\n[14] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned\\none-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision\\n(ICCV) , pages 3490–3499. IEEE Computer Society, 2021.\\n[15] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha.\\nRank diminishing in deep neural networks. Advances in Neural Information Processing Systems ,\\n35:33054–33065, 2022.\\n[16] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in\\n2021. arXiv preprint arXiv:2107.08430 , 2021.\\n[17] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V\\nLe, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance\\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 2918–2928, 2021.\\n10', '[18] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer\\nvision , pages 1440–1448, 2015.\\n[19] Jocher Glenn. Yolov5 release v7.0. https: // github. com/ ultralytics/ yolov5/ tree/\\nv7. 0 , 2022.\\n[20] Jocher Glenn. Yolov8. https: // github. com/ ultralytics/ ultralytics/ tree/\\nmain , 2023.\\n[21] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé\\nJégou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference.\\nInProceedings of the IEEE/CVF international conference on computer vision , pages 12259–\\n12269, 2021.\\n[22] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of\\nthe IEEE international conference on computer vision , pages 2961–2969, 2017.\\n[23] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression.\\nInProceedings of the IEEE conference on computer vision and pattern recognition , pages\\n4507–4515, 2017.\\n[24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural\\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.\\n[25] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\\npages 3588–3597, 2018.\\n[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\\nby reducing internal covariate shift. In International conference on machine learning , pages\\n448–456. pmlr, 2015.\\n[27] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming\\nXu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading. arXiv preprint arXiv:2301.05586 ,\\n2023.\\n[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate\\ndetr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 13619–13627, 2022.\\n[29] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss\\nv2: Learning reliable localization quality estimation for dense object detection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition , pages 11632–11641,\\n2021.\\n[30] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang.\\nGeneralized focal loss: Learning qualified and distributed bounding boxes for dense object\\ndetection. Advances in Neural Information Processing Systems , 33:21002–21012, 2020.\\n[31] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design\\nfor gpu-efficient networks. arXiv preprint arXiv:2006.14090 , 2020.\\n[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\\nobject detection. In Proceedings of the IEEE international conference on computer vision ,\\npages 2980–2988, 2017.\\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 740–755. Springer, 2014.\\n[34] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang.\\nDab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329 ,\\n2022.\\n11', '[35] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for\\ninstance segmentation. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pages 8759–8768, 2018.\\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\\nof the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021.\\n[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\\nXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 11976–11986, 2022.\\n[38] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive\\nfield in deep convolutional neural networks. Advances in neural information processing systems ,\\n29, 2016.\\n[39] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong\\nZhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors.\\narXiv preprint arXiv:2212.07784 , 2022.\\n[40] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and\\nJingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF\\ninternational conference on computer vision , pages 3651–3660, 2021.\\n[41] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review\\nof statistics and its application , 6:405–431, 2019.\\n[42] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/\\ndarknet/ , 2013–2016.\\n[43] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\\nreal-time object detection. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) , June 2016.\\n[44] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) , July 2017.\\n[45] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 4510–4520, 2018.\\n[47] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and\\nJian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings\\nof the IEEE/CVF international conference on computer vision , pages 8430–8439, 2019.\\n[48] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in\\ncrowded scenes. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pages 2325–2333, 2016.\\n[49] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What\\nmakes for end-to-end object detection? In International Conference on Machine Learning ,\\npages 9934–9944. PMLR, 2021.\\n[50] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,\\nLei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with\\nlearnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 14454–14463, 2021.\\n[51] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object\\ndetector. IEEE Transactions on Pattern Analysis and Machine Intelligence , 44(4):1922–1933,\\n2020.\\n12', '[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[53] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile\\ncnn from vit perspective. arXiv preprint arXiv:2307.09283 , 2023.\\n[54] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai\\nHan. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in\\nNeural Information Processing Systems , 36, 2024.\\n[55] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling\\ncross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and\\npattern recognition , pages 13029–13038, 2021.\\n[56] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-\\nfreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pages 7464–7475, 2023.\\n[57] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and\\nI-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition workshops , pages\\n390–391, 2020.\\n[58] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies\\nthrough gradient path analysis. arXiv preprint arXiv:2211.04800 , 2022.\\n[59] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to\\nlearn using programmable gradient information. arXiv preprint arXiv:2402.13616 , 2024.\\n[60] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end\\nobject detection with fully convolutional network. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 15849–15858, 2021.\\n[61] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for\\ntransformer-based detector. In Proceedings of the AAAI conference on artificial intelligence ,\\nvolume 36, pages 2567–2575, 2022.\\n[62] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\\nIntroducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international\\nconference on computer vision , pages 22–31, 2021.\\n[63] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with\\nattention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875 , 2023.\\n[64] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong\\nWang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo.\\narXiv preprint arXiv:2203.16250 , 2022.\\n[65] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo:\\nA report on real-time object detection design. arXiv preprint arXiv:2211.15444 , 2022.\\n[66] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr:\\nEnd-to-end multiple-object tracking with transformer. In European Conference on Computer\\nVision , pages 659–675. Springer, 2022.\\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-\\nYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.\\narXiv preprint arXiv:2203.03605 , 2022.\\n[68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\\nempirical risk minimization. arXiv preprint arXiv:1710.09412 , 2017.\\n13', '[69] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between\\nanchor-based and anchor-free detection via adaptive training sample selection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition , pages 9759–9768,\\n2020.\\n[70] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu,\\nGang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic\\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 12083–12093, 2022.\\n[71] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu,\\nand Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069 ,\\n2023.\\n[72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou\\nloss: Faster and better learning for bounding box regression. In Proceedings of the AAAI\\nconference on artificial intelligence , volume 34, pages 12993–13000, 2020.\\n[73] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms.\\nIEEE Transactions on Multimedia , 2023.\\n[74] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 ,\\n2020.\\n[75] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training.\\nInProceedings of the IEEE/CVF international conference on computer vision , pages 6748–6758,\\n2023.\\n14', 'A Appendix\\nA.1 Implementation Details\\nFollowing [ 20,56,59], all YOLOv10 models are trained from scratch using the SGD optimizer for\\n500 epochs. The SGD momentum and weight decay are set to 0.937 and 5 ×10−4, respectively. The\\ninitial learning rate is 1 ×10−2and it decays linearly to 1 ×10−4. For data augmentation, we adopt the\\nMosaic [ 2,19], Mixup [ 68] and copy-paste augmentation [ 17],etc., like [ 20,59]. Tab. 14 presents the\\ndetailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase\\nthe width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the\\nSPPF module [ 20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion\\nratio of 2 for the inverted bottleneck block structure. Following [ 59,56], we report the standard mean\\naverage precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\\nMoreover, we follow [ 71] to establish the end-to-end speed benchmark. Since the execution time\\nof NMS is affected by the input, we thus measure the latency on the COCO valset, like [ 71]. We\\nadopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT\\nefficientNMSPlugin is appended for post-processing and the I/Ooverhead is omitted. We report\\nthe average latency across all images.\\nTable 14: Hyper-parameters of YOLOv10.\\nhyper-parameter YOLOv10-N/S/M/B/L/X\\nepochs 500\\noptimizer SGD\\nmomentum 0.937\\nweight decay 5 ×10−4\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\ninitial learning rate 10−2\\nfinal learning rate 10−4\\nlearning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015\\ntranslation augmentation 0.1\\nscale augmentation 0.5/0.5/0.9/0.9/0.9/0.9\\nmosaic augmentation 1.0\\nMixup augmentation 0.0/0.0/0.1/0.1/0.15/0.15\\ncopy-paste augmentation 0.0/0.0/0.1/0.1/0.3/0.3\\nclose mosaic epochs 10\\nA.2 Details of Consistent Matching Metric\\nWe provide the detailed derivation of consistent matching metric here.\\nAs mentioned in the paper, we suppose that the one-to-many positive samples is Ωand the one-to-\\none branch selects i-th prediction. We can then leverage the normalized metric [ 14] to obtain the\\nclassification target for task alignment learning [20, 14, 59, 27, 64], i.e.,to2m,j=u∗·mo2m,j\\nm∗\\no2m≤u∗\\nforj∈Ωandto2o,i=u∗·mo2o,i\\nm∗\\no2o=u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\\nA=|(1−to2o,i)−(1−I(i∈Ω)to2m,i)|+X\\nk∈Ω\\\\{i}|1−(1−to2m,k)|\\n=|to2o,i−I(i∈Ω)to2m,i|+X\\nk∈Ω\\\\{i}to2m,k\\n=to2o,i−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k,(3)\\n15', 'where I(·)is the indicator function. We denote the classification targets of the predictions in Ωas\\n{ˆt1,ˆt2, ...,ˆt|Ω|}in descending order, with ˆt1≥ˆt2≥...≥ˆt|Ω|. We can then replace to2o,iwithu∗\\nand obtain:\\nA=u∗−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k\\n=u∗+X\\nk∈Ωto2m,k−2·I(i∈Ω)to2m,i\\n=u∗+X|Ω|\\nk=1ˆtk−2·I(i∈Ω)to2m,i(4)\\nWe further discuss the supervision gap in two scenarios, i.e.,\\n1. Supposing i̸∈Ω, we can obtain:\\nA=u∗+X|Ω|\\nk=1ˆtk (5)\\n2. Supposing i∈Ω, we denote to2m,i=ˆtnand obtain:\\nA=u∗+X|Ω|\\nk=1ˆtk−2·ˆtn (6)\\nDue to ˆtn≥0, the second case can lead to smaller supervision gap. Besides, we can observe that A\\ndecreases as ˆtnincreases, indicating that ndecreases and the ranking of iwithin Ωimproves. Due\\ntoˆtn≤ˆt1,Athus achieves the minimum when ˆtn=ˆt1,i.e.,iis the best positive sample in Ωwith\\nmo2m,i=m∗\\no2mandto2m,i=u∗·mo2m,i\\nm∗\\no2m=u∗.\\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching\\nmetric. We suppose αo2m>0andβo2m>0, which are common in [ 20,59,27,14,64]. Similarly,\\nwe assume αo2o>0andβo2o>0. We can obtain r1=αo2o\\nαo2m>0andr2=βo2o\\nβo2m>0, and then\\nderive mo2oby\\nmo2o=s·pαo2o·IoU(ˆb, b)βo2o\\n=s·pr1·αo2m·IoU(ˆb, b)r2·βo2m\\n=s·(pαo2m·IoU(ˆb, b)βo2m)r1·IoU(ˆb, b)(r2−r1)·βo2m\\n=mr1\\no2m·IoU(ˆb, b)(r2−r1)·βo2m(7)\\nTo achieve mo2m,i=m∗\\no2mandmo2o,i=m∗\\no2o, we can make mo2omonotonically increase with\\nmo2mby assigning (r2−r1) = 0 ,i.e.,\\nmo2o=mr1\\no2m·IoU(ˆb, b)0·βo2m\\n=mr1\\no2m(8)\\nSupposing r1=r2=r, we can thus derive the consistent matching metric, i.e.,αo2o=r·αo2mand\\nβo2o=r·βo2m. By simply taking r= 1, we obtain αo2o=αo2mandβo2o=βo2m.\\nA.3 Details of Rank-Guided Block Design\\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate\\nthe numerical rank of the convolution, we reshape its weight to the shape of ( Co,K2×Ci), where Co\\nandCidenote the number of output and input channels, and Kmeans the kernel size, respectively.\\nA.4 More Results on COCO\\nWe report the detailed performance of YOLOv10 on COCO, including APval\\n50and APval\\n75at different\\nIoU thresholds, as well as APval\\nsmall , APval\\nmedium , and APval\\nlarge across different scales, in Tab. 15.\\nA.5 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\\nWe note that reducing the latency of YOLOv10-S (#2 in Tab. 2) is particularly challenging due to its\\nsmall model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a\\n5.3% reduction in latency without compromising performance. This provides substantial support for\\nthe further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off\\nwith our holistic efficiency-accuracy driven model design, showing a 2.0% AP improvement with only\\n16', 'Algorithm 1: Rank-guided block design\\nInput: Intrinsic ranks Rfor all stages S; Original Network Θ; CIB θcib;\\nOutput: New network Θ∗with CIB for certain stages.\\n1t←0;\\n2Θ0←Θ;Θ∗←Θ0;\\n3ap0←AP(T(Θ0)); // T:training the network; AP:evaluating the AP performance.\\n4while S̸=∅do\\n5 st←argmins∈SR;\\n6 Θt+1←Replace (Θt, θcib,st);// Replace the block in Stage stofΘtwith CIB θcib.\\n7 apt+1←AP(T(Θt+1));\\n8 ifapt+1≥ap0then\\n9 Θ∗←Θt+1;S←S\\\\ {st};\\n10 else\\n11 return Θ∗;\\n12 end\\n13end\\n14return Θ∗;\\nTable 15: Detailed performance of YOLOv10 on COCO.\\nModel APval(%) APval\\n50(%) APval\\n75(%) APval\\nsmall (%) APval\\nmedium (%) APval\\nlarge (%)\\nYOLOv10-N 38.5 53.8 41.7 18.9 42.4 54.6\\nYOLOv10-S 46.3 63.0 50.4 26.8 51.0 63.8\\nYOLOv10-M 51.1 68.1 55.8 33.8 56.5 67.0\\nYOLOv10-B 52.5 69.6 57.2 35.1 57.8 68.5\\nYOLOv10-L 53.2 70.1 58.1 35.8 58.5 69.4\\nYOLOv10-X 54.4 71.3 59.3 37.0 59.8 70.9\\n0.05ms latency overhead. Besides, for YOLOv10-M (#6 in Tab. 2), which has a larger model scale\\nand more redundancy, our efficiency driven model design results in a considerable 12.5% latency\\nreduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a\\nnotable 0.8% AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48ms.\\nThese results well demonstrate the effectiveness of our design strategy across different model scales.\\nA.6 Visualization Results\\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It\\ncan be observed that YOLOv10 can achieve precise detection under various difficult conditions, such\\nas low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely\\npacked objects, such as bottle, cup, and person. These results indicate its superior performance.\\nA.7 Contribution, Limitation, and Broader Impact\\nContribution. In summary, our contributions are three folds as follows:\\n1.We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label\\nassignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious\\nsupervision between two branches, we innovatively propose the consistent matching metric, which\\ncan well reduce the theoretical supervision gap and lead to improved performance.\\n2.We propose a holistic efficiency-accuracy driven model design strategy for the model architecture\\nof YOLOs. We present novel lightweight classification head, spatial-channel decoupled down-\\nsampling, and rank-guided block design, which greatly reduce the computational redundancy and\\nachieve high efficiency. We further introduce the large-kernel convolution and innovative partial\\nself-attention module, which effectively enhance the performance under low cost.\\n3.Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object\\ndetector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art\\nperformance and efficiency trade-offs compared with other advanced detectors.\\n17', 'Figure 4: Visualization results under complex and challenging scenarios.\\nLimitation. Due to the limited computational resources, we do not investigate the pretraining\\nof YOLOv10 on large-scale datasets, e.g., Objects365 [ 47]. Besides, although we can achieve\\ncompetitive end-to-end performance using the one-to-one head under NMS-free training, there still\\nexists a performance gap compared with the original one-to-many training using NMS, especially\\nnoticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of\\none-to-many training with NMS outperforms that of NMS-free training by 1.0% AP and 0.5% AP,\\nrespectively. We will explore ways to further reduce the gap and achieve higher performance for\\nYOLOv10 in the future work.\\nBroader impact. The YOLOs can be widely applied in various real-world applications, including\\nmedical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these\\nfields and improve the efficiency. However, we acknowledge the potential for malicious use of our\\nmodels. We will make every effort to prevent this.\\n18']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for doc in docs:\n",
    "  content = doc.page_content\n",
    "  corpus.append(content)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "iq_a6U2Uifc8",
    "outputId": "b7dd75b8-f4d2-44e6-c3ce-9cf1665d0436"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'YOLOv10: Real-Time End-to-End Object Detection\\nAo Wang Hui Chen∗Lihao Liu Kai Chen Zijia Lin\\nJungong Han Guiguang Ding∗\\nTsinghua University\\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\\n/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000011/uni00000018/uni00000017/uni00000018/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000018/uni00000011/uni00000013/uni00000026/uni00000032/uni00000026/uni00000032/uni00000003/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000019/uni00000010/uni00000059/uni00000016/uni00000011/uni00000013\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001a\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001b\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni0000001c\\n/uni00000033/uni00000033/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000028\\n/uni00000035/uni00000037/uni00000030/uni00000027/uni00000048/uni00000057\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000010/uni00000030/uni00000036\\n/uni0000002a/uni00000052/uni0000004f/uni00000047/uni00000010/uni0000003c/uni00000032/uni0000002f/uni00000032\\n/uni00000035/uni00000037/uni00000010/uni00000027/uni00000028/uni00000037/uni00000035\\n/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000059/uni00000014/uni00000013/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right)\\ntrade-offs. We measure the end-to-end latency using the official pre-trained models.\\nAbstract\\nOver the past years, YOLOs have emerged as the predominant paradigm in the field\\nof real-time object detection owing to their effective balance between computa-\\ntional cost and detection performance. Researchers have explored the architectural\\ndesigns, optimization objectives, data augmentation strategies, and others for YO-\\nLOs, achieving notable progress. However, the reliance on the non-maximum\\nsuppression (NMS) for post-processing hampers the end-to-end deployment of\\nYOLOs and adversely impacts the inference latency. Besides, the design of various\\ncomponents in YOLOs lacks the comprehensive and thorough inspection, resulting\\nin noticeable computational redundancy and limiting the model’s capability. It ren-\\nders the suboptimal efficiency, along with considerable potential for performance\\nimprovements. In this work, we aim to further advance the performance-efficiency\\nboundary of YOLOs from both the post-processing and the model architecture. To\\nthis end, we first present the consistent dual assignments for NMS-free training of\\nYOLOs, which brings the competitive performance and low inference latency simul-\\ntaneously. Moreover, we introduce the holistic efficiency-accuracy driven model\\ndesign strategy for YOLOs. We comprehensively optimize various components of\\nYOLOs from both the efficiency and accuracy perspectives, which greatly reduces\\nthe computational overhead and enhances the capability. The outcome of our effort\\nis a new generation of YOLO series for real-time end-to-end object detection,\\ndubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-\\nof-the-art performance and efficiency across various model scales. For example,\\nour YOLOv10-S is 1.8 ×faster than RT-DETR-R18 under the similar AP on COCO,\\nmeanwhile enjoying 2.8 ×smaller number of parameters and FLOPs. Compared\\nwith YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters\\nfor the same performance. Code: https://github.com/THU-MIG/yolov10 .\\n∗Corresponding Author.\\nPreprint. Under review.arXiv:2405.14458v1  [cs.CV]  23 May 2024'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl2xQnupidy8",
    "outputId": "4c9f1253-8c12-45c0-9903-5966273fbc39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "94cdcda320434fb09fa729207949a8b7",
      "8012770bb93f4e48b0af7344c96f1d32",
      "4e07621fa38146beac75fa0959a134b9",
      "77840f8d476c4970b9053f1c26ca9a73",
      "b45215058363447caccd0687a247f336",
      "718a80e58f5a40aa8c9f1ea8765922cd",
      "d9b19c5b459a45e38326c2a777388654",
      "bc61ade33c4d4988b6efce0fc61a4a91",
      "4ba46573f51948a3846025ca0918855d",
      "dc56f4fd5f7749059504ead3071ad2ff",
      "820c8af3c58d45519cf9a6dac40fc80a"
     ]
    },
    "id": "02elANomgQ8W",
    "outputId": "2800caa5-1205-4a67-ef71-3731f4a2338f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cdcda320434fb09fa729207949a8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit tf-idf values on your corpus\n",
    "bm25_encoder.fit(corpus)\n",
    "\n",
    "# store the values to a json file\n",
    "bm25_encoder.dump(\"bm25_values.json\")\n",
    "\n",
    "# load to your BM25Encoder object\n",
    "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "QgzYY8DUghG9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MDe_8FJiwgX"
   },
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "_PJlphUjjeGC"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "\n",
    "retriever = PineconeHybridSearchRetriever(\n",
    "    embeddings=embeddings,\n",
    "    sparse_encoder=bm25_encoder,\n",
    "    index=index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QklKWJRslLx0",
    "outputId": "58953d62-5901-4bb5-894c-657746aa42ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PineconeHybridSearchRetriever(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7ce41001c0d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7ce3557f3a60>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), sparse_encoder=<pinecone_text.sparse.bm25_encoder.BM25Encoder object at 0x7ce4101ff0a0>, index=<pinecone.data.index.Index object at 0x7ce410204d90>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFHkY8wGjuXI"
   },
   "source": [
    "### Use the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "302f8c92a783422ab95479d83bb53d6f",
      "f9aff0e79a3f4ea69bc24906ed52703e",
      "57725fb790794524b45b6988f30230de",
      "4b17da9018414a0ba74ced81d26ffb87",
      "1061f725f0cf495483c3d86eb4c82c37",
      "4173b3c309fd42168d639089472a6aaa",
      "40d6c502b8d64440b7b8f540d7686b78",
      "e3fbc0fc72514b3c84ddc4d14bced910",
      "3147a86a2cff466e8e1b0b9a47f1b231",
      "faa8065a02d745cca70afacd76d36b44",
      "3ea1d36bf8524606b1fbf0df08bdca72"
     ]
    },
    "id": "gzntpnLSlRn4",
    "outputId": "bee9a649-c149-43fc-8ac1-6b38caef381e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302f8c92a783422ab95479d83bb53d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.add_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "EgxjJoGujuKN"
   },
   "outputs": [],
   "source": [
    "result = retriever.invoke(\"Implementation Details of Yolov10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcCPDvbfjpf0",
    "outputId": "0118af4e-66f9-4bc1-de9a-399978012c56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Appendix\\nA.1 Implementation Details\\nFollowing [ 20,56,59], all YOLOv10 models are trained from scratch using the SGD optimizer for\\n500 epochs. The SGD momentum and weight decay are set to 0.937 and 5 ×10−4, respectively. The\\ninitial learning rate is 1 ×10−2and it decays linearly to 1 ×10−4. For data augmentation, we adopt the\\nMosaic [ 2,19], Mixup [ 68] and copy-paste augmentation [ 17],etc., like [ 20,59]. Tab. 14 presents the\\ndetailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase\\nthe width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the\\nSPPF module [ 20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion\\nratio of 2 for the inverted bottleneck block structure. Following [ 59,56], we report the standard mean\\naverage precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\\nMoreover, we follow [ 71] to establish the end-to-end speed benchmark. Since the execution time\\nof NMS is affected by the input, we thus measure the latency on the COCO valset, like [ 71]. We\\nadopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT\\nefficientNMSPlugin is appended for post-processing and the I/Ooverhead is omitted. We report\\nthe average latency across all images.\\nTable 14: Hyper-parameters of YOLOv10.\\nhyper-parameter YOLOv10-N/S/M/B/L/X\\nepochs 500\\noptimizer SGD\\nmomentum 0.937\\nweight decay 5 ×10−4\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\ninitial learning rate 10−2\\nfinal learning rate 10−4\\nlearning rate schedule linear decay\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\nHSV hue augmentation 0.015\\ntranslation augmentation 0.1\\nscale augmentation 0.5/0.5/0.9/0.9/0.9/0.9\\nmosaic augmentation 1.0\\nMixup augmentation 0.0/0.0/0.1/0.1/0.15/0.15\\ncopy-paste augmentation 0.0/0.0/0.1/0.1/0.3/0.3\\nclose mosaic epochs 10\\nA.2 Details of Consistent Matching Metric\\nWe provide the detailed derivation of consistent matching metric here.\\nAs mentioned in the paper, we suppose that the one-to-many positive samples is Ωand the one-to-\\none branch selects i-th prediction. We can then leverage the normalized metric [ 14] to obtain the\\nclassification target for task alignment learning [20, 14, 59, 27, 64], i.e.,to2m,j=u∗·mo2m,j\\nm∗\\no2m≤u∗\\nforj∈Ωandto2o,i=u∗·mo2o,i\\nm∗\\no2o=u∗. We can thus derive the supervision gap between two\\nbranches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\\nA=|(1−to2o,i)−(1−I(i∈Ω)to2m,i)|+X\\nk∈Ω\\\\{i}|1−(1−to2m,k)|\\n=|to2o,i−I(i∈Ω)to2m,i|+X\\nk∈Ω\\\\{i}to2m,k\\n=to2o,i−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k,(3)\\n15'),\n",
       " Document(page_content='mixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, e.g., embedded in the\\nELAN structure [ 58,20] (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to\\nachieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we\\nsort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance\\nvariation of replacing the basic block in the leading stage with CIB. If there is no performance\\ndegradation compared with the given model, we proceed with the replacement of the next stage and\\nhalt the process otherwise. Consequently, we can implement adaptive compact block designs across\\nstages and model scales, achieving higher efficiency without compromising performance. Due to the\\npage limit, we provide the details of the algorithm in the appendix.\\nAccuracy driven model design. We further explore the large-kernel convolution and self-attention\\nfor accuracy driven design, aiming to boost the performance under minimal cost.\\n(1) Large-kernel convolution. Employing large-kernel depthwise convolution is an effective way to\\nenlarge the receptive field and enhance the model’s capability [ 9,38,37]. However, simply leveraging\\nthem in all stages may introduce contamination in shallow features used for detecting small objects,\\nwhile also introducing significant I/Ooverhead and latency in high-resolution stages [ 7]. Therefore,\\nwe propose to leverage the large-kernel depthwise convolutions in CIB within the deep stages.\\nSpecifically, we increase the kernel size of the second 3 ×3 depthwise convolution in the CIB to 7 ×7,\\nfollowing [ 37]. Additionally, we employ the structural reparameterization technique [ 10,9,53] to\\nbring another 3 ×3 depthwise convolution branch to alleviate the optimization issue without inference\\noverhead. Furthermore, as the model size increases, its receptive field naturally expands, with\\nthe benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel\\nconvolution for small model scales.\\n(2) Partial self-attention (PSA). Self-attention [ 52] is widely employed in various visual tasks due\\nto its remarkable global modeling capability [ 36,13,70]. However, it exhibits high computational\\ncomplexity and memory footprint. To address this, in light of the prevalent attention head redun-\\ndancy [ 63], we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c).\\nSpecifically, we evenly partition the features across channels into two parts after the 1 ×1 convolution.\\nWe only feed one part into the NPSAblocks comprised of multi-head self-attention module (MHSA)\\nand feed-forward network (FFN). Two parts are then concatenated and fused by a 1 ×1 convolution.\\nBesides, we follow [ 21] to assign the dimensions of the query and key to half of that of the value in\\nMHSA and replace the LayerNorm [1] with BatchNorm [26] for fast inference. Furthermore, PSA is\\nonly placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the\\nquadratic computational complexity of self-attention. In this way, the global representation learning\\nability can be incorporated into YOLOs with low computational costs, which well enhances the\\nmodel’s capability and leads to improved performance.\\n4 Experiments\\n4.1 Implementation Details\\nWe select YOLOv8 [ 20] as our baseline model, due to its commendable latency-accuracy balance\\nand its availability in various model sizes. We employ the consistent dual assignments for NMS-free\\ntraining and perform holistic efficiency-accuracy driven model design based on it, which brings our\\nYOLOv10 models. YOLOv10 has the same variants as YOLOv8, i.e., N / S / M / L / X. Besides, we\\nderive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We\\nverify the proposed detector on COCO [ 33] under the same training-from-scratch setting [ 20,59,56].\\nMoreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [71].\\n4.2 Comparison with state-of-the-arts\\nAs shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency\\nacross various model scales. We first compare YOLOv10 with our baseline models, i.e., YOLOv8.\\nOn N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP\\nimprovements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38%\\nless calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,\\nYOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically,\\nfor lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0\\n6'),\n",
       " Document(page_content='where I(·)is the indicator function. We denote the classification targets of the predictions in Ωas\\n{ˆt1,ˆt2, ...,ˆt|Ω|}in descending order, with ˆt1≥ˆt2≥...≥ˆt|Ω|. We can then replace to2o,iwithu∗\\nand obtain:\\nA=u∗−I(i∈Ω)to2m,i+X\\nk∈Ω\\\\{i}to2m,k\\n=u∗+X\\nk∈Ωto2m,k−2·I(i∈Ω)to2m,i\\n=u∗+X|Ω|\\nk=1ˆtk−2·I(i∈Ω)to2m,i(4)\\nWe further discuss the supervision gap in two scenarios, i.e.,\\n1. Supposing i̸∈Ω, we can obtain:\\nA=u∗+X|Ω|\\nk=1ˆtk (5)\\n2. Supposing i∈Ω, we denote to2m,i=ˆtnand obtain:\\nA=u∗+X|Ω|\\nk=1ˆtk−2·ˆtn (6)\\nDue to ˆtn≥0, the second case can lead to smaller supervision gap. Besides, we can observe that A\\ndecreases as ˆtnincreases, indicating that ndecreases and the ranking of iwithin Ωimproves. Due\\ntoˆtn≤ˆt1,Athus achieves the minimum when ˆtn=ˆt1,i.e.,iis the best positive sample in Ωwith\\nmo2m,i=m∗\\no2mandto2m,i=u∗·mo2m,i\\nm∗\\no2m=u∗.\\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching\\nmetric. We suppose αo2m>0andβo2m>0, which are common in [ 20,59,27,14,64]. Similarly,\\nwe assume αo2o>0andβo2o>0. We can obtain r1=αo2o\\nαo2m>0andr2=βo2o\\nβo2m>0, and then\\nderive mo2oby\\nmo2o=s·pαo2o·IoU(ˆb, b)βo2o\\n=s·pr1·αo2m·IoU(ˆb, b)r2·βo2m\\n=s·(pαo2m·IoU(ˆb, b)βo2m)r1·IoU(ˆb, b)(r2−r1)·βo2m\\n=mr1\\no2m·IoU(ˆb, b)(r2−r1)·βo2m(7)\\nTo achieve mo2m,i=m∗\\no2mandmo2o,i=m∗\\no2o, we can make mo2omonotonically increase with\\nmo2mby assigning (r2−r1) = 0 ,i.e.,\\nmo2o=mr1\\no2m·IoU(ˆb, b)0·βo2m\\n=mr1\\no2m(8)\\nSupposing r1=r2=r, we can thus derive the consistent matching metric, i.e.,αo2o=r·αo2mand\\nβo2o=r·βo2m. By simply taking r= 1, we obtain αo2o=αo2mandβo2o=βo2m.\\nA.3 Details of Rank-Guided Block Design\\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate\\nthe numerical rank of the convolution, we reshape its weight to the shape of ( Co,K2×Ci), where Co\\nandCidenote the number of output and input channels, and Kmeans the kernel size, respectively.\\nA.4 More Results on COCO\\nWe report the detailed performance of YOLOv10 on COCO, including APval\\n50and APval\\n75at different\\nIoU thresholds, as well as APval\\nsmall , APval\\nmedium , and APval\\nlarge across different scales, in Tab. 15.\\nA.5 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\\nWe note that reducing the latency of YOLOv10-S (#2 in Tab. 2) is particularly challenging due to its\\nsmall model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a\\n5.3% reduction in latency without compromising performance. This provides substantial support for\\nthe further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off\\nwith our holistic efficiency-accuracy driven model design, showing a 2.0% AP improvement with only\\n16'),\n",
       " Document(page_content='Algorithm 1: Rank-guided block design\\nInput: Intrinsic ranks Rfor all stages S; Original Network Θ; CIB θcib;\\nOutput: New network Θ∗with CIB for certain stages.\\n1t←0;\\n2Θ0←Θ;Θ∗←Θ0;\\n3ap0←AP(T(Θ0)); // T:training the network; AP:evaluating the AP performance.\\n4while S̸=∅do\\n5 st←argmins∈SR;\\n6 Θt+1←Replace (Θt, θcib,st);// Replace the block in Stage stofΘtwith CIB θcib.\\n7 apt+1←AP(T(Θt+1));\\n8 ifapt+1≥ap0then\\n9 Θ∗←Θt+1;S←S\\\\ {st};\\n10 else\\n11 return Θ∗;\\n12 end\\n13end\\n14return Θ∗;\\nTable 15: Detailed performance of YOLOv10 on COCO.\\nModel APval(%) APval\\n50(%) APval\\n75(%) APval\\nsmall (%) APval\\nmedium (%) APval\\nlarge (%)\\nYOLOv10-N 38.5 53.8 41.7 18.9 42.4 54.6\\nYOLOv10-S 46.3 63.0 50.4 26.8 51.0 63.8\\nYOLOv10-M 51.1 68.1 55.8 33.8 56.5 67.0\\nYOLOv10-B 52.5 69.6 57.2 35.1 57.8 68.5\\nYOLOv10-L 53.2 70.1 58.1 35.8 58.5 69.4\\nYOLOv10-X 54.4 71.3 59.3 37.0 59.8 70.9\\n0.05ms latency overhead. Besides, for YOLOv10-M (#6 in Tab. 2), which has a larger model scale\\nand more redundancy, our efficiency driven model design results in a considerable 12.5% latency\\nreduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a\\nnotable 0.8% AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48ms.\\nThese results well demonstrate the effectiveness of our design strategy across different model scales.\\nA.6 Visualization Results\\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It\\ncan be observed that YOLOv10 can achieve precise detection under various difficult conditions, such\\nas low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely\\npacked objects, such as bottle, cup, and person. These results indicate its superior performance.\\nA.7 Contribution, Limitation, and Broader Impact\\nContribution. In summary, our contributions are three folds as follows:\\n1.We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label\\nassignments way is designed to provide rich supervision by one-to-many branch during training\\nand high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious\\nsupervision between two branches, we innovatively propose the consistent matching metric, which\\ncan well reduce the theoretical supervision gap and lead to improved performance.\\n2.We propose a holistic efficiency-accuracy driven model design strategy for the model architecture\\nof YOLOs. We present novel lightweight classification head, spatial-channel decoupled down-\\nsampling, and rank-guided block design, which greatly reduce the computational redundancy and\\nachieve high efficiency. We further introduce the large-kernel convolution and innovative partial\\nself-attention module, which effectively enhance the performance under low cost.\\n3.Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object\\ndetector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art\\nperformance and efficiency trade-offs compared with other advanced detectors.\\n17')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yd2CVgfj0KE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDw5uKjymPeK"
   },
   "source": [
    "# Chain\n",
    "\n",
    "Memory - Prompt - StrOutputParser - Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "pUPGnkT0mV58"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "m_sINN7CnwDX"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZOGNWiRp1G5"
   },
   "source": [
    "### Defining System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "7poFg3-1pXhI"
   },
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"\n",
    "      You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer\n",
    "      the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the\n",
    "      answer concise.\n",
    "      \\n\\n\n",
    "      {context}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGJl4E2Dp5sk"
   },
   "source": [
    "### Contextualizing the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "fe_KXPrQryfa"
   },
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"\"\"\n",
    "    Given a chat history and the latest user question which might reference context in the chat history,\n",
    "    formulate a standalone question which can be understood without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "O1IjiNWfqLSZ"
   },
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1vZ7jOFqQCH"
   },
   "source": [
    "### Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "V0I8j_GzokKM"
   },
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_hTK-DEsORr"
   },
   "source": [
    "### Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBvB2hj2qb8r",
    "outputId": "228d47cc-f6e4-458f-ab7d-f42ddf043615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Yolo?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "print(chat_history)\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "XOmpKcSJqn-A",
    "outputId": "0b06c28e-0160-4d0e-b6c4-46fa5edc7e97"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'YOLO, which stands for \"You Only Look Once,\" is a popular real-time object detection algorithm that processes images in a single pass, enabling fast and efficient identification of objects and their locations within an image. It is widely used in various applications, including autonomous driving and video surveillance, due to its balance between speed and accuracy. The architecture involves a single neural network that predicts bounding boxes and class probabilities directly from full images, making it distinct from traditional methods that require separate region proposals.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epvQkyleqqkr",
    "outputId": "79ca6209-cd92-412d-d48c-48f6e6834d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is Yolo?'), AIMessage(content='YOLO, which stands for \"You Only Look Once,\" is a popular real-time object detection algorithm that processes images in a single pass, enabling fast and efficient identification of objects and their locations within an image. It is widely used in various applications, including autonomous driving and video surveillance, due to its balance between speed and accuracy. The architecture involves a single neural network that predicts bounding boxes and class probabilities directly from full images, making it distinct from traditional methods that require separate region proposals.')]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HvBX2_Kq5hq",
    "outputId": "fc96604c-3c57-4402-b098-28e7dc54688e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 is trained from scratch using the SGD optimizer for 500 epochs, with a momentum of 0.937 and a weight decay of 5 × 10^-4. The initial learning rate is set to 0.01 and decays linearly to 0.0001, with various data augmentation techniques such as Mosaic, Mixup, and copy-paste employed. Additionally, the model is trained on 8 NVIDIA 3090 GPUs, and hyper-parameters are meticulously detailed, including box loss gain, class loss gain, and various augmentation parameters.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the implemenattion detail of Yolov10?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E0ruyjfrFmm",
    "outputId": "35767ff6-fa5b-4e46-dde8-3142ddc23a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusion of the YOLOv10 paper emphasizes the development of a new generation of real-time end-to-end object detectors that effectively balance performance and efficiency. By addressing post-processing inefficiencies and optimizing model architecture, YOLOv10 achieves significant improvements in both accuracy and latency compared to previous models. Extensive experiments demonstrate its superiority in state-of-the-art performance across various model scales, indicating its potential for practical applications in real-time object detection.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the conclusion we obtain from  Yolov10 paper?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FswEwc96rbLp",
    "outputId": "8f6d95d2-5467-420e-96a4-5066cbad197a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMS stands for Non-Maximum Suppression, a technique used in object detection to eliminate redundant bounding boxes around detected objects. It works by selecting the box with the highest confidence score and removing any other boxes that overlap significantly with it based on a defined threshold. This process helps to ensure that only the most accurate and relevant predictions are retained for final output, improving the quality of detection results.\n"
     ]
    }
   ],
   "source": [
    "question = \"What does NMS mean?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zie_C_g3sb7d",
    "outputId": "b26482cb-9359-4e59-949e-626a90dedebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the paper, YOLOv10 outperforms several previous models, including YOLOv8, YOLOv6, and RT-DETR. Specifically, it achieves state-of-the-art performance across various model scales, demonstrating significant improvements in accuracy, latency, and parameter efficiency compared to these models. The paper highlights that YOLOv10-S and YOLOv10-X are 1.8× and 1.3× faster than RT-DETR-R18 and R101, respectively, while also outpacing YOLOv8 in terms of mean average precision (AP).\n"
     ]
    }
   ],
   "source": [
    "question = \"Based on the paper, Yolov10 outperforms how many other model and which are those?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKYKjmN6s1Y_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1061f725f0cf495483c3d86eb4c82c37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "302f8c92a783422ab95479d83bb53d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9aff0e79a3f4ea69bc24906ed52703e",
       "IPY_MODEL_57725fb790794524b45b6988f30230de",
       "IPY_MODEL_4b17da9018414a0ba74ced81d26ffb87"
      ],
      "layout": "IPY_MODEL_1061f725f0cf495483c3d86eb4c82c37"
     }
    },
    "3147a86a2cff466e8e1b0b9a47f1b231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ea1d36bf8524606b1fbf0df08bdca72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40d6c502b8d64440b7b8f540d7686b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4173b3c309fd42168d639089472a6aaa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b17da9018414a0ba74ced81d26ffb87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faa8065a02d745cca70afacd76d36b44",
      "placeholder": "​",
      "style": "IPY_MODEL_3ea1d36bf8524606b1fbf0df08bdca72",
      "value": " 1/1 [00:02&lt;00:00,  2.90s/it]"
     }
    },
    "4ba46573f51948a3846025ca0918855d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e07621fa38146beac75fa0959a134b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc61ade33c4d4988b6efce0fc61a4a91",
      "max": 18,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ba46573f51948a3846025ca0918855d",
      "value": 18
     }
    },
    "57725fb790794524b45b6988f30230de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3fbc0fc72514b3c84ddc4d14bced910",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3147a86a2cff466e8e1b0b9a47f1b231",
      "value": 1
     }
    },
    "718a80e58f5a40aa8c9f1ea8765922cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77840f8d476c4970b9053f1c26ca9a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc56f4fd5f7749059504ead3071ad2ff",
      "placeholder": "​",
      "style": "IPY_MODEL_820c8af3c58d45519cf9a6dac40fc80a",
      "value": " 18/18 [00:00&lt;00:00, 77.67it/s]"
     }
    },
    "8012770bb93f4e48b0af7344c96f1d32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_718a80e58f5a40aa8c9f1ea8765922cd",
      "placeholder": "​",
      "style": "IPY_MODEL_d9b19c5b459a45e38326c2a777388654",
      "value": "100%"
     }
    },
    "820c8af3c58d45519cf9a6dac40fc80a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94cdcda320434fb09fa729207949a8b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8012770bb93f4e48b0af7344c96f1d32",
       "IPY_MODEL_4e07621fa38146beac75fa0959a134b9",
       "IPY_MODEL_77840f8d476c4970b9053f1c26ca9a73"
      ],
      "layout": "IPY_MODEL_b45215058363447caccd0687a247f336"
     }
    },
    "b45215058363447caccd0687a247f336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc61ade33c4d4988b6efce0fc61a4a91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9b19c5b459a45e38326c2a777388654": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc56f4fd5f7749059504ead3071ad2ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3fbc0fc72514b3c84ddc4d14bced910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9aff0e79a3f4ea69bc24906ed52703e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4173b3c309fd42168d639089472a6aaa",
      "placeholder": "​",
      "style": "IPY_MODEL_40d6c502b8d64440b7b8f540d7686b78",
      "value": "100%"
     }
    },
    "faa8065a02d745cca70afacd76d36b44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
