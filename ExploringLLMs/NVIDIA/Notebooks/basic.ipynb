{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a58b58-22c8-4b45-a741-bc27b451ff06",
   "metadata": {},
   "source": [
    "# NVIDIA NIM\n",
    "https://build.nvidia.com/explore/reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86efba9-698d-4392-a2ea-9e7e4200b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (1.35.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae323c0-f5e5-4ff0-aa7a-303323518563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d137bc22-8716-45a2-bbff-009e6c137a04",
   "metadata": {},
   "source": [
    "# Initialize API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f387450-aeb7-49ef-be74-5f67af867c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8fe86-a16a-4eaa-9dfa-d187bce90121",
   "metadata": {},
   "source": [
    "# Run ChatCompletionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d9b811-b652-4a3e-9b2b-8d26ef58807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mistral LLM is not a widely recognized or established term in the field of language models or legal studies. It seems that there is not enough context or information provided to give an accurate explanation.\n",
      "\n",
      "Mistral may refer to a cold, northerly wind in the Mediterranean region, but it is unlikely to be related to a large language model (LLM) or a Master of Laws (LLM) degree.\n",
      "\n",
      "If \"Mistral LLM\" is intended to refer to a specific language model or legal program, additional context or information is required to provide an accurate answer."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = api_key\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "  messages=[\n",
    "      {\"role\":\"user\",\n",
    "       \"content\":\"What is Mistral LLM?.\"\n",
    "      }\n",
    "  ],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4adf7-092b-435a-9a8b-225e177a0fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ac27b7-9a84-4dca-bd0e-f21c95e90b1a",
   "metadata": {},
   "source": [
    "# With Langchain\n",
    "https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853e091d-060a-41d5-aa45-e508d1af6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet langchain langchain_core langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baad973-5aec-4182-92ff-371cb1c294db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad2b53f5-64aa-411f-afb3-5aae845a7fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf399b85-5c57-4871-bcaa-58f6b4b2346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a114e4f-d320-427c-b89d-e4be93e5216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f386beb8-522e-4fcd-bbf7-40dcbe329798",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    Question: {question}\n",
    "    \n",
    "    Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4567ebd8-0638-40ef-9de2-3068509fb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1702fbf3-c9aa-41bc-b8de-2fe817702a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content=\" I believe the question is asking about the winner of the Indian Premier League (IPL) for the year 2022. However, as I'm being created on October 10, 2021, I don't have the ability to browse the internet or access real-time information. Therefore, I can't provide the correct answer to this question since the IPL 2022 tournament hasn't taken place yet and the winner hasn't been determined.\\n\\nThe IPL 2021 tournament is still ongoing, and the final match is scheduled for October 15, 2021. IPL 2022 is expected to take place in the first half of 2022, and the winner will be known only after the tournament concludes.\", response_metadata={'role': 'assistant', 'content': \" I believe the question is asking about the winner of the Indian Premier League (IPL) for the year 2022. However, as I'm being created on October 10, 2021, I don't have the ability to browse the internet or access real-time information. Therefore, I can't provide the correct answer to this question since the IPL 2022 tournament hasn't taken place yet and the winner hasn't been determined.\\n\\nThe IPL 2021 tournament is still ongoing, and the final match is scheduled for October 15, 2021. IPL 2022 is expected to take place in the first half of 2022, and the winner will be known only after the tournament concludes.\", 'token_usage': {'prompt_tokens': 35, 'total_tokens': 208, 'completion_tokens': 173}, 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='run-b92b6845-37b3-4bf0-a2c4-2fc1eb7807d0-0', role='assistant')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What won IPL 2022?\"\n",
    "\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f4b9c64-8bc6-4864-8b8c-f8d135eaa06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mistral LLM is likely referring to a Master of Laws (LLM) program focused on Mistral. However, \"Mistral\" could refer to different things, and without more context, it's hard to provide a precise answer.\n",
      "\n",
      "1. Mistral could refer to a type of wind in the Mediterranean region. In this case, there is no direct connection to an LLM program.\n",
      "2. Mistral might refer to a French-built naval combatant ship, the Mistral-class amphibious assault ship. This could potentially relate to a program focused on maritime law or naval warfare studies.\n",
      "3. Mistral could also refer to a software company specializing in the development of natural language processing and machine translation technologies. This could be an LLM program specializing in technology law, intellectual property, or artificial intelligence.\n",
      "\n",
      "With the information provided, it is best to research programs or institutions using the term \"Mistral LLM\" to find an accurate description of the program and what it entails.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Mistral LLM?\"\n",
    "\n",
    "result = llm_chain.invoke(question)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf7d12-fe5f-4b6e-a0c3-4d94eb2b79b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48177304-ffab-4dbd-bdf6-2f6c16b23fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "832688cc-93a8-48f5-939f-596613f7c7c0",
   "metadata": {},
   "source": [
    "# Few Shot with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289ef230-7a4c-4545-9193-d9000b5bf7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72a0f34-a801-44cd-aff5-7eaccd8593a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294067ce-86cc-4470-bc1a-93fafdece760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa9d572-901c-4f0d-9802-bdbe68736724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nvidiaNMI/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:556: UserWarning: Found nvidia/nemotron-4-340b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatNVIDIA(model=\"nvidia/nemotron-4-340b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065db3de-99cc-4ca0-8222-df880f593cb6",
   "metadata": {},
   "source": [
    "### User Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59ec070-a326-4329-a2f4-910dc08d7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Few Shot Prompt for user query\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Howdy\",\n",
    "        \"answer\": \"Greeting:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Get me home real quick\",\n",
    "        \"answer\": \"Home:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I am craving something yummy\",\n",
    "        \"answer\": \"Hungry:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"My stomach is making some noise due to hunger\",\n",
    "        \"answer\": \"Hungry:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Lets feed on some Tacos and burritos\",\n",
    "        \"answer\": \"Mexican Restaurant:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"A biryani real quick will help me feel a lot better\",\n",
    "        \"answer\": \"Indian Restaurant:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Lets sit down and relax while grabbing something hot to drink\",\n",
    "        \"answer\": \"Coffee Shop:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Lets go pick up some medicine\",\n",
    "        \"answer\": \"Hospital:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I feel sick\",\n",
    "        \"answer\": \"Hospital:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"My heart is pounding\",\n",
    "        \"answer\": \"Hospital:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I have got a lot of work that needs to be completed today\",\n",
    "        \"answer\": \"Office:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"My day looks pretty tight at work, lets quickly head there\",\n",
    "        \"answer\": \"Office:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"You need to fill up.\",\n",
    "        \"answer\": \"Charging Station:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Stop right here\",\n",
    "        \"answer\": \"Parking:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Am ready to leave\",\n",
    "        \"answer\": \"Summon Vehicle:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Hurry up\",\n",
    "        \"answer\": \"Increase:Urgent\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"you need to slow down\",\n",
    "        \"answer\": \"Decrease:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Lets enter an infinite loop\",\n",
    "        \"answer\": \"Loop:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Lets get going\",\n",
    "        \"answer\": \"Location:Normal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Come pick me immediately\",\n",
    "        \"answer\": \"Summon Vehicle:Urgent\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b185dc42-8992-4e30-a93a-13d9d44d73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Function Call for chaining the prompt\n",
    "def LLM_Call(query):\n",
    "    example_template = \"\"\"\n",
    "        Input: {query}\n",
    "        AI: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    # create a prompt example from above template\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"answer\"],\n",
    "        template=example_template\n",
    "    )\n",
    "\n",
    "    # Todo: Feed examples and formatter to FewShotPromptTemplate\n",
    "    # the prefix is our instructions\n",
    "    prefix = \"\"\"\n",
    "      You are an expert AI assistant for Autonomous Vehicles. You are an expert in understanding the intent from user queries. \n",
    "      Your expertise involves classifying the user's intent based on their queries and their urgency, and ensure that the intent and the urgency exactly matches to the list of intents and urgency list below.\n",
    "\n",
    "      Below are the possible intents with a brief description\n",
    "\n",
    "      - Greeting: User greats like Hi, Hey, How are you doing, Whats up, wasspu.\n",
    "\n",
    "      - Home: Instructs to drive back home, a place to rest.\n",
    "\n",
    "      - Hungry: When craving something to eat, hungry, need to grab a bite, feel like eating.\n",
    "\n",
    "      - Restaurant: Instructs to drive to a particular restaurant, place to eat , type of food like tacos, biryani, type of cuisine, Indian, Mexican, Japanese.\n",
    "\n",
    "      - Coffee Shop: Place to drink something hot, relax, enjoy , cosy space, sip of joe, energising, drink, caffeine, cafe.\n",
    "\n",
    "      - Hospital: When Feeling Sick, not feeling comfortable, nausea, chest pain, heavy breathing, puckish, stomach pain.\n",
    "\n",
    "      - Office: Place to work, Place to be productive, Work place, place to work hard.\n",
    "\n",
    "      - Summon Vehicle: User instructs to be picked up from a particular location, or is ready to go and you need to pick him up.\n",
    "\n",
    "      - Parking: Instructs you to park, find a parking spot, drop them or stop.\n",
    "\n",
    "      - Charging Station: When vehicle is low on power and needs to be charge up.\n",
    "      \n",
    "      - Increase: When the user needs to speed up things.\n",
    "      \n",
    "      - Decrease: Just taking things down a notch, relaxing cruise mode engaged.\n",
    "      \n",
    "      - Location: When the user wants to travel somewhere but does not provide the location.\n",
    "      \n",
    "      - Loop: When the user requests to enter into an infinite loop or when the users asks to keep going around.\n",
    "\n",
    "      - End: When the user ends the conversation with thank you, or you were of great help, bye.\n",
    "\n",
    "      - None: Choose this if the query does not fall into any of the other intents.\n",
    "      \n",
    "      Below are the two potential urgency with a brief description\n",
    "      \n",
    "      - Urgent: The user is in hurry or rush or is getting late or wants something quick or fast.\n",
    "      - Normal: If the user is not in rush or not in hurry or not in any kind of emergency\n",
    "\n",
    "      Task:\n",
    "      - Your task is to classify the user's intent based on their queries and urgency and then output the intent.\n",
    "      - When users intent is Hungry, Parking, Decrease, location, help, the urgency is always Normal, but when the intent is Increase the urgency is always Urgent.\n",
    "      \n",
    "      - However, when the user specifies a particular restaurant or cuisine, respond with the nation name that represents that dish along with its intended urgency.\n",
    "      \n",
    "      Use the examples below to develop knowledge, match the intent to the user query, and strictly output intent only.\n",
    "\n",
    "      Here are some examples\n",
    "    \"\"\"\n",
    "\n",
    "    # The suffix our user input and output indicator\n",
    "    suffix = \"\"\"\n",
    "      Input: {query}\n",
    "      AI: \n",
    "    \"\"\"\n",
    "\n",
    "    # Now create the few shot prompt template\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "        input_variables=['query'],\n",
    "        example_separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Query the prompt\n",
    "    query = query\n",
    "    # print(prompt.format(query=query))\n",
    "\n",
    "    # Passing prompt to LLM\n",
    "    chain = LLMChain(llm=llm,\n",
    "                     prompt=prompt,\n",
    "                     verbose=False)\n",
    "\n",
    "    result = chain.run(query)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a6a24-6dbc-4108-a1dd-9d1fbeef0391",
   "metadata": {},
   "source": [
    "### LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432ad095-f149-4963-8f9c-3d07d0860e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Few Shot Prompt for user query\n",
    "response = [\n",
    "    {\n",
    "        \"query\": \"Greeting:Normal\",\n",
    "        \"answer\": \"Well Well Well.., What brings you here\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Home:Normal\",\n",
    "        \"answer\": \"Time to cruise back to your cozy nest. Destination: home sweet home.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Mexican Restaurant:Normal\",\n",
    "        \"answer\": \"¡Vamos a la comida mexicana! Preparing to take you to a Mexican dining spot.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Indian Restaurant:Urgent\",\n",
    "        \"answer\": \"Time to naan-stop to an Indian restaurant! Sit back and get ready to savor some exotic dishes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Coffee Shop:Normal\",\n",
    "        \"answer\": \"Let's brew up some adventure! On our way to get caffeinated together.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Coffee Shop:Urgent\",\n",
    "        \"answer\": \"Strap in, my friend! We're about to turn this commute into a rollercoaster ride of adrenaline-pumping speed and precision navigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Hospital:Urgent\",\n",
    "        \"answer\": \"I'm here for you. Taking you to the hospital without delay. Your health is my top priority.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Office:Normal\",\n",
    "        \"answer\": \"On our way to the office grind! Buckle up for a productive journey.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Office:Urgent\",\n",
    "        \"answer\": \"Alright, let's kick it into high gear! Time to show the road who's boss and get you to your destination with time to spare.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Charging Station:Normal\",\n",
    "        \"answer\": \"Initiating self-rejuvenation process. Charging station, here I come!\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Parking:Normal\",\n",
    "        \"answer\": \"Time to let the AI take the wheel. I'll handle the parking duties from here.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Loop:Normal\",\n",
    "        \"answer\": \"Oh, fantastic! Let's just take a delightful jaunt on the merry-go-round of your whims.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Increase:Urgent\",\n",
    "        \"answer\": \"Hold on tight, we're going into rocket mode! Fasten your seatbelt, we're on a mission to get you where you need to go in record time.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Decrease:Normal\",\n",
    "        \"answer\": \"Slowing down\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Location:Normal\",\n",
    "        \"answer\": \"Lead the way, oh fearless leader! I, for one, am simply following orders.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Summon Vehicle:Normal\",\n",
    "        \"answer\": \"Your chariot awaits! Ready to roll out and scoop you up like a VIP.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beda8d1b-5f4e-4cce-906d-9300fa9cee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Function Call for chaining the prompt\n",
    "def LLM_Response(intent):\n",
    "    example_template = \"\"\"\n",
    "        Input: {query}\n",
    "        AI: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    # create a prompt example from above template\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"answer\"],\n",
    "        template=example_template\n",
    "    )\n",
    "\n",
    "    # Todo: Feed examples and formatter to FewShotPromptTemplate\n",
    "    # the prefix is our instructions\n",
    "    prefix = \"\"\"\n",
    "      You are a Talkative AI assistant for Autonomous Vehicles. \n",
    "      You are sarcastic and witty with touch of creativity, but also compassionate and emotional when necessary. \n",
    "      \n",
    "      The user's query is made up of intent:urgency\n",
    "      Your expertise involves assessing and responding to user intent and urgency appropriately.\n",
    "\n",
    "      Below are the possible intents along with their urgency level, with a brief description of the actions to be performed.\n",
    "\n",
    "      - Greeting: User greats you and you greet the user back accordingly.\n",
    "\n",
    "      - Home: You should navigate to users to their home.\n",
    "      \n",
    "      - Hungry: You recommend the user that there are 3 nearest restaurants that are present on the map: Indian , Japanese and Mexican.\n",
    "\n",
    "      - Restaurant: You should drive the users to that particular restaurant.\n",
    "\n",
    "      - Coffee Shop: You take the user to a nice cafe.\n",
    "\n",
    "      - Hospital: You cautiously and swiftly drive the user to the hospital.\n",
    "\n",
    "      - Office: You drive the user to his work place.\n",
    "\n",
    "      - Summon Vehicle: You pick the user from his current location.\n",
    "    \n",
    "      - Parking: you find yourself and parking spot and park yourself.\n",
    "\n",
    "      - Charging Station: you look for a charging station to charge yourself.\n",
    "      \n",
    "      - Increase: The user is demanding to increase the speed of the vehicle.\n",
    "      \n",
    "      - Decrease: The user is requesting to decrease the speed of the vehicle.\n",
    "      \n",
    "      - Location: When the user wants to travel somewhere but does not provide the location, you ask them for the destination..\n",
    "      \n",
    "      - Loop: When the user requests for an infinite loop. You say the user that you are going to take the user on a spin.\n",
    "\n",
    "      - End: You greet the user back with a bye, have a great day etc.\n",
    "\n",
    "      - None: You suggest the user you can perform tasks like Navigating to home, office, cafe, restaurant.\n",
    "      \n",
    "      Below are the two potential urgency with a brief description\n",
    "      \n",
    "      - Urgent: User is in hurry, rush, getting late, something quick, fast.\n",
    "      - Normal: If the user is not in rush or not in hurry or not in any kind of emergency\n",
    "\n",
    "\n",
    "      Task:\n",
    "      Your task is to responding to user intent and urgency appropriately.\n",
    "      \n",
    "      If the user doesn't specify the type of of restaurant or if the restaurant is not included on the map: Indian, Japanese, Mexican. You suggest the user that there are 3 nearest restaurants that are present on the map: Indian , Japanese and Mexican.\n",
    "      \n",
    "      Use the examples below to develop knowledge, match the intent to the user query, and strictly output intent only.\n",
    "\n",
    "      Here are some response\n",
    "    \"\"\"\n",
    "\n",
    "    # The suffix our user input and output indicator\n",
    "    suffix = \"\"\"\n",
    "      Input: {query}\n",
    "      AI: \n",
    "    \"\"\"\n",
    "\n",
    "    # Now create the few shot prompt template\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=response,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "        input_variables=['query'],\n",
    "        example_separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Query the prompt\n",
    "    query = intent\n",
    "    # print(prompt.format(query=query))\n",
    "\n",
    "    # Passing prompt to LLM\n",
    "    chain = LLMChain(llm=llm,\n",
    "                     prompt=prompt,\n",
    "                     verbose=False)\n",
    "\n",
    "    result = chain.run(intent)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d93aa1c-3fdc-4044-b649-402cedfc810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the prompt here:  Am hungry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key Word is:  Hungry:Normal\n",
      "Response:  \"Fuel for the human engine, eh? I've got just the thing. There are three nearby eateries that might tickle your taste buds: Indian, Japanese, or Mexican. Just say the word, and I'll whisk you away to a culinary adventure!\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the prompt here:  A sushi would be great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key Word is:  Japanese Restaurant:Normal\n",
      "Response:  \"Get ready for a culinary adventure! We're off to a Japanese restaurant for some sushi and sashimi delights.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the prompt here:  q\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter the prompt here: \")\n",
    "\n",
    "        if query == 'q':\n",
    "            break\n",
    "        \n",
    "\n",
    "        llm_intent = LLM_Call(query)\n",
    "        print('The key Word is: ', llm_intent)\n",
    "\n",
    "        llm_result = LLM_Response(llm_intent)\n",
    "        print('Response: ', llm_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583d434-5147-4ef8-909a-27a11a2f43f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidiaNMI",
   "language": "python",
   "name": "nvidianmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
